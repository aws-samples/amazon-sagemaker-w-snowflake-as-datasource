{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bdcbdd3",
   "metadata": {},
   "source": [
    "# Snowflake as Data Source for training an ML Model with Amazon Sagemaker\n",
    "**_Use of Snowflake Data Table as Data Source for training a Sagemaker Model without having Snowflake Data to stage on S3_**\n",
    "\n",
    "This notebook works well with the `conda_python3` kernel on a SageMaker Notebook `ml.t3.xlarge` instance.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Objective](#Objective)\n",
    "1. [Background](#Background-(Problem-Description-and-Approach))\n",
    "1. [Create a Training Script](#Create-a-training-script)\n",
    "1. [Define `Model` Hyperparameters](#Define-Model-Hyperparameters)\n",
    "1. [Launch a training job with Python SDK](#Launch-a-training-job-with-Python-SDK)\n",
    "1. [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4334f4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "This notebook illustrates how to retrieve data stored in a [Snowflake](https://www.snowflake.com/) table and use it for training an ML model using Amazon SageMaker _without having to first store the data in S3_. \n",
    "\n",
    "This example uses the [California Housing dataset (provided by Scikit-Learn)](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html) and trains a XGBoost model to predict house prices. A detailed description about the dataset can be found [here](https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html).\n",
    "\n",
    "To understand the code, you might also find it useful to refer to:\n",
    "\n",
    "- *The guide on [Use XGBoost with the SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/frameworks/xgboost/using_xgboost.html#)*\n",
    "- *Docker Registry Paths [Docker Registry Paths and Example Code](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html)*\n",
    "- *The [SageMaker reference for Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#client) (The general AWS SDK for Python, including low-level bindings for SageMaker as well as many other AWS services)*\n",
    "---\n",
    "\n",
    "## Background (Problem Description and Approach)\n",
    "\n",
    "- **Problem statement**: SageMaker requires the training data to be present either in [S3 or in EFS or in FSX for Lustre](https://aws.amazon.com/blogs/machine-learning/choose-the-best-data-source-for-your-amazon-sagemaker-training-job/). In order to train a model using data stored outside of the three supported storage services, the data first needs to be ingested into one of these services (typically S3). This requires building a data pipeline (using tools such as [Amazon SageMaker Data Wrangler](https://aws.amazon.com/sagemaker/data-wrangler/)) to move data into S3. However, this may create a data management challenge in some situations (data lifecycle management, access control etc.) and it may be desirable to have the data accessible to SageMaker _without_ the intermediate storage of data into S3. This notebook illustrates a way to do this using Snowflake as a 3rd party data source.\n",
    "\n",
    "- **Our approach**: Launch a SageMaker Training Job using the SageMaker SDK with a custom training script and have the training script download the data from Snowflake directly into the instance created for running the training job thus avoiding the temporary storage of data in S3. Use the [Snowflake Connector for Python](https://docs.snowflake.com/en/user-guide/python-connector.html) to connect and download the data from the Snowflake. Once the data is downloaded in the training instance then the training script proceeds to train an ML model using the scikit-learn SDK. **Note that it is assumed that the data is already available in Snowflake, see [`snowflake instructions`](./snowflake-instructions.md) for instructions on creating a database in Snowflake and ingesting the California Housing dataset as a table.**\n",
    "\n",
    "- **Our tools**: [Amazon SageMaker SDK](https://sagemaker.readthedocs.io/en/stable/), [Snowflake Connector for Python](https://docs.snowflake.com/en/user-guide/python-connector.html) and [SageMaker XGBoost Estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/xgboost/using_xgboost.html#).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb65b13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Overall Workflow\n",
    "\n",
    "The overall workflow for this notebook is shown in the diagram below.\n",
    "\n",
    "![sagemaker snowflake](img/sm-snowflake-1p.png)\n",
    "\n",
    "Steps 1 and 2 are executed outside of this notebook. \n",
    "\n",
    "1. See [`snowflake instructions`](./snowflake-instructions.md) for instructions on creating a database in Snowflake and ingesting the California Housing dataset as a table.\n",
    "1. See [`secrets manager instructions`](./secretsmanager-instructions.md) for instructions on storing Snowflake credentials.\n",
    "\n",
    "The following sections in this notebook describe the rest of the steps i.e. downloading data from Snowflake and training the XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2255b24a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create directories for code files (training script and others)\n",
    "\"\"\"\n",
    "!mkdir src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948931fa",
   "metadata": {},
   "source": [
    "### Script for downloading credentials\n",
    "\n",
    "This script is written to a file locally in a directory called `src`. The `src` directory name is provided as the `source_dir` parameter to the `Estimator`, all files written to the `src` directory are made available to the training container on startup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729a61de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/snowflake_credentials.py\n",
    "\n",
    "\"\"\"\n",
    "Retrieve Snowflake password for given username from AWS SecretsManager\n",
    "\"\"\"\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "def get_credentials(secret_id: str, region_name: str) -> str:\n",
    "    \n",
    "    client = boto3.client('secretsmanager', region_name=region_name)\n",
    "    response = client.get_secret_value(SecretId=secret_id)\n",
    "    secrets_value = json.loads(response['SecretString'])    \n",
    "    \n",
    "    return secrets_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02664082",
   "metadata": {},
   "source": [
    "## `requirements.txt` for packages our training script will need\n",
    "\n",
    "We will create a `requirements.txt` file and list all the dependencies in this file. The SageMaker Training job will install these dependencies on startup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610050a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/requirements.txt\n",
    "\n",
    "boto3==1.26.44\n",
    "sagemaker==2.127.0\n",
    "snowflake-connector-python==2.9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910747a4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Train an XGBoost Regressor using SageMaker Training Jobs\n",
    "\n",
    "Now we are ready to train our ML model using SageMaker Training Jobs. We do this in the following steps:\n",
    "\n",
    "1. Create separate Python scripts for connecting to Snowflake, querying (downloading) the data, preparing the data for ML and finally a training scripts which ties everything together.\n",
    "\n",
    "1. Provide the training script to the SageMaker SDK [Estimator](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) along with the source directory so that all the scripts we create can be provided to the training container when the training job is run using the [Estimator.fit](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#sagemaker.estimator.EstimatorBase.fit) method.\n",
    "\n",
    "**NOTE: the data from Snowflake is downloaded directly into the training container instance and at no point is it stored in S3.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85212c16",
   "metadata": {},
   "source": [
    "### Create a Snowflake connection Script\n",
    "\n",
    "Use the [Snowflake Connector for Python](https://docs.snowflake.com/en/user-guide/python-connector.html) to connect and download the data from the Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29cc8d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/connection.py\n",
    "\n",
    "\"\"\"\n",
    "Establish connection with Snowflake table - HOUSING\n",
    "\"\"\"\n",
    "\n",
    "import snowflake.connector\n",
    "import snowflake_credentials\n",
    "from snowflake_credentials import get_credentials\n",
    "\n",
    "def connect(secret_id: str, account: str, warehouse: str, database: str, schema: str, protocol: str, region: str) -> snowflake.connector.SnowflakeConnection:\n",
    "    \n",
    "    secret_value = get_credentials(secret_id, region)\n",
    "    sf_user = secret_value['username']\n",
    "    sf_password = secret_value['password']\n",
    "    sf_account = account\n",
    "    sf_warehouse = warehouse\n",
    "    sf_database = database\n",
    "    sf_schema = schema\n",
    "    sf_protocol = protocol\n",
    "    \n",
    "    print(f\"sf_user={sf_user}, sf_password=****, sf_account={sf_account}, sf_warehouse={sf_warehouse}, \"\n",
    "          f\"sf_database={sf_database}, sf_schema={sf_schema}, sf_protocol={sf_protocol}\")    \n",
    "    \n",
    "    # Read to connect to snowflake\n",
    "    ctx = snowflake.connector.connect(user=sf_user,\n",
    "                                      password=sf_password,\n",
    "                                      account=sf_account,\n",
    "                                      warehouse=sf_warehouse,\n",
    "                                      database=sf_database,\n",
    "                                      schema=sf_schema,\n",
    "                                      protocol=sf_protocol)\n",
    "    \n",
    "    # Once the connection is established we read the dataset (table)\n",
    "    # into a dataframe\n",
    "    cs=ctx.cursor()\n",
    "    print(\"\\nSnowflake connection established...\")\n",
    "    \n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882ba0e3",
   "metadata": {},
   "source": [
    "### Create a Snowflake Querying Script\n",
    "\n",
    "To query data records from Snowflake database table, and store it in a dataframe.\n",
    "\n",
    "**For distributed data parallel training we download a random subset of data into each training instance. Each training instance downloads an equal amount of data which is simply `total number of rows/ total number of training hosts`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2d5833",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/query_snowflake.py\n",
    "\n",
    "\"\"\"\n",
    "Read the HOUSING table (this is the california housing dataset used by this example)\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "\n",
    "def data_pull(ctx: snowflake.connector.SnowflakeConnection, table: str, hosts: int) -> pd.DataFrame:\n",
    "    \n",
    "    # Query Snowflake HOUSING table for number of table records\n",
    "    sql_cnt = f\"select count(*) from {table};\"\n",
    "    df_cnt = pd.read_sql(sql_cnt, ctx)\n",
    "\n",
    "    # Retrieve the total number of table records from dataframe\n",
    "    for index, row in df_cnt.iterrows():\n",
    "        num_of_records = row.astype(int)\n",
    "        list_num_of_rec = num_of_records.tolist()\n",
    "    tot_num_records = list_num_of_rec[0]\n",
    "\n",
    "    record_percent = str(round(100/hosts))\n",
    "    print(f\"going to download a random {record_percent}% sample of the data\")\n",
    "    # Query Snowflake HOUSING table\n",
    "    sql = f\"select * from {table} sample ({record_percent});\"\n",
    "    print(f\"sql={sql}\")\n",
    "    \n",
    "    # Get the dataset into Pandas\n",
    "    df = pd.read_sql(sql, ctx)\n",
    "    print(f\"read data into a dataframe of shape {df.shape}\")\n",
    "    # Prepare the data for ML\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    print(f\"final shape of dataframe to be used for training {df.shape}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0c5a91",
   "metadata": {},
   "source": [
    "### Create a Data Preparation Script\n",
    "\n",
    "The input dataframe is split into training and test datasets using [SKlearn's train_test_split function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) The split datasets are then converted to [XGB DMatrices](https://xgboost.readthedocs.io/en/stable/python/python_api.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce0e51b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/data_preparation.py\n",
    "\n",
    "\"\"\"\n",
    "Preparation of training and test datasets for XGBoost Model\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prepare_data(df: pd.DataFrame) -> tuple:\n",
    "    \n",
    "    # preparation of training and test datasets\n",
    "    X = df.drop(['MEDHOUSEVAL'], axis=1)  \n",
    "    y = df['MEDHOUSEVAL']\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "    \n",
    "    print(\"building training and testing datasets\")\n",
    "    features = X.select_dtypes('number').columns.tolist()\n",
    "    print(f\"features={features}\")\n",
    "    X_train = X_train[features]\n",
    "    X_test = X_test[features]\n",
    "\n",
    "    # -- MODEL TRAINING --    \n",
    "    print(\"going to train the model\")\n",
    "    \n",
    "    # Converting input datasets to XGB DMatrices for XGBoost Model Training\n",
    "    dtrain = xgb.DMatrix(X_train.values, y_train.values)\n",
    "    dval = xgb.DMatrix(X_test.values, y_test.values)\n",
    "    watchlist = [(dtrain, 'train'), (dval, 'validation')] if dval is not None else [(dtrain, 'train')]\n",
    "    \n",
    "    return dtrain, dval, watchlist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dfc695",
   "metadata": {},
   "source": [
    "### Create a Training Script\n",
    "\n",
    "The [SageMaker Scikit-Learn Framework Container](https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-docker-containers-scikit-learn-spark.html) provides the basic runtime, and we provide a custom [`training script`](./src/train.py) that contains the actual ML training code. The folder containing the training script also contains a [`requirements.txt`](./src/requirements.txt) file to specify any additional dependencies that need to be installed in the training instance.\n",
    "\n",
    "You can find detailed guidance in the documentation on [Preparing a Scikit-Learn training script](https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/using_sklearn.html#prepare-a-scikit-learn-training-script) (for training).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bc48db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/train.py\n",
    "\n",
    "\"\"\"\n",
    "Train a Sagemaker XGBoost Model on the california housing dataset\n",
    "\"\"\"\n",
    "\n",
    "# Import dependencies\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import argparse\n",
    "import pickle as pkl\n",
    "import xgboost as xgb\n",
    "from connection import connect\n",
    "from query_snowflake import data_pull\n",
    "from data_preparation import prepare_data\n",
    "from sagemaker_containers import entry_point\n",
    "from sagemaker_xgboost_container import distributed\n",
    "from sagemaker_xgboost_container.data_utils import get_dmatrix\n",
    "\n",
    "def _xgb_train(params: dict, dtrain: xgb.DMatrix, evals: list, num_boost_round: int, model_dir: str, is_master: bool) -> None:\n",
    "    \"\"\"Run xgb train on arguments given with rabit initialized.\n",
    "\n",
    "    This is our rabit execution function.\n",
    "\n",
    "    :param args_dict: Argument dictionary used to run xgb.train().\n",
    "    :param is_master: True if current node is master host in distributed training,\n",
    "                        or is running single node training job.\n",
    "                        Note that rabit_run includes this argument.\n",
    "    \"\"\"\n",
    "    booster = xgb.train(params=params,\n",
    "                        dtrain=dtrain,\n",
    "                        evals=evals,\n",
    "                        num_boost_round=num_boost_round)\n",
    "\n",
    "    if is_master:\n",
    "        model_location = model_dir + '/xgboost-model'\n",
    "        pkl.dump(booster, open(model_location, 'wb'))\n",
    "        logging.info(\"Stored trained model at {}\".format(model_location))\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyperparameters are described here.\n",
    "    parser.add_argument('--max_depth', type=int,)\n",
    "    parser.add_argument('--eta', type=float)\n",
    "    parser.add_argument('--gamma', type=int)\n",
    "    parser.add_argument('--min_child_weight', type=int)\n",
    "    parser.add_argument('--subsample', type=float)\n",
    "    parser.add_argument('--verbosity', type=int)\n",
    "    parser.add_argument('--objective', type=str)\n",
    "    parser.add_argument('--num_round', type=int)\n",
    "    parser.add_argument('--tree_method', type=str, default=\"auto\")\n",
    "    parser.add_argument('--predictor', type=str, default=\"auto\")\n",
    "\n",
    "    # Sagemaker specific arguments. Defaults are set in the environment variables.\n",
    "    parser.add_argument('--output_data_dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--validation', type=str, default=os.environ.get('SM_CHANNEL_VALIDATION'))\n",
    "    parser.add_argument('--sm_hosts', type=str, default=os.environ.get('SM_HOSTS'))\n",
    "    parser.add_argument('--sm_current_host', type=str, default=os.environ.get('SM_CURRENT_HOST'))\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # Get SageMaker host information from runtime environment variables\n",
    "    sm_hosts = json.loads(args.sm_hosts)\n",
    "    sm_current_host = args.sm_current_host   \n",
    "    \n",
    "    # snowflake related params are read from environment variables\n",
    "    secret_id = os.environ[\"SECRET_ID\"]\n",
    "    account = os.environ[\"SF_ACCOUNT\"]\n",
    "    warehouse = os.environ[\"SF_WAREHOUSE\"]\n",
    "    database = os.environ[\"SF_DATABASE\"].upper()\n",
    "    schema = os.environ[\"SF_SCHEMA\"].upper()\n",
    "    table = os.environ['SF_TABLE'].upper()\n",
    "    region = os.environ[\"AWS_REGION\"]\n",
    "    \n",
    "    protocol = \"https\"\n",
    "    \n",
    "    # Connect to Snowflake database table \n",
    "    ctx = connect(secret_id, account, warehouse, database, schema, protocol, region)\n",
    "    \n",
    "    # Query data from Snowflake database table\n",
    "    # IMPORTANT: DATA FROM SNOWFLAKE GOES DIRECTLY INTO PANDA DF. THE DATA DOES __NOT__ GET STAGED IN AN S3 BUCKET. \n",
    "    df = data_pull(ctx, table, len(sm_hosts))\n",
    "    \n",
    "    # Preparation of training and test datasets\n",
    "    dtrain, dval, watchlist = prepare_data(df)\n",
    "    \n",
    "    # Define training hyperparameters\n",
    "    train_hp = {\n",
    "        'max_depth': args.max_depth,\n",
    "        'eta': args.eta,\n",
    "        'gamma': args.gamma,\n",
    "        'min_child_weight': args.min_child_weight,\n",
    "        'subsample': args.subsample,\n",
    "        'verbosity': args.verbosity,\n",
    "        'objective': args.objective,\n",
    "        'tree_method': args.tree_method,\n",
    "        'predictor': args.predictor,\n",
    "    }\n",
    "    \n",
    "    xgb_train_args = dict(\n",
    "        params=train_hp,\n",
    "        dtrain=dtrain,\n",
    "        evals=watchlist,\n",
    "        num_boost_round=args.num_round,\n",
    "        model_dir=args.model_dir)\n",
    "\n",
    "    if len(sm_hosts) > 1:\n",
    "        # Wait until all hosts are able to find each other\n",
    "        entry_point._wait_hostname_resolution()\n",
    "\n",
    "        # Execute training function after initializing rabit.\n",
    "        distributed.rabit_run(\n",
    "            exec_fun=_xgb_train,\n",
    "            args=xgb_train_args,\n",
    "            include_in_training=(dtrain is not None),\n",
    "            hosts=sm_hosts,\n",
    "            current_host=sm_current_host,\n",
    "            update_rabit_args=True\n",
    "        )\n",
    "    else:\n",
    "        # If single node training, call training method directly.\n",
    "        if dtrain:\n",
    "            xgb_train_args['is_master'] = True\n",
    "            _xgb_train(**xgb_train_args)\n",
    "        else:\n",
    "            raise ValueError(\"Training channel must have data to train model.\")\n",
    "\n",
    "\n",
    "def model_fn(model_dir: str):\n",
    "    \"\"\"Deserialize and return fitted model.\n",
    "\n",
    "    Note that this should have the same name as the serialized model in the _xgb_train method\n",
    "    \"\"\"\n",
    "    model_file = 'xgboost-model'\n",
    "    booster = pkl.load(open(os.path.join(model_dir, model_file), 'rb'))\n",
    "    return booster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd32564",
   "metadata": {},
   "source": [
    "#### Provide the Snowflake username and connection details as environment variables to the Training container\n",
    "\n",
    "Retrieve the Snowflake credentials from AWS Secrets Mananger. This is done by the training code and we just need to provide the secrets identifier as an environment variable.\n",
    "\n",
    "We also need to retrieve your account identifier for Snowflake which we already obtained when we run the previous notebook (snowflake-load-dataset.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a8d500",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r sf_account_id \n",
    "%store -r sf_secret_id \n",
    "print(f\"sf_account_id={sf_account_id}, sf_secret_id={sf_secret_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809447f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# do not change!!!\n",
    "# the values of these variables match what we put in the snowflake-load-dataset.ipynb file\n",
    "warehouse = \"amazon_sagemake_w_snowflake_as_datasource\"\n",
    "database = \"housing\"\n",
    "schema = \"housing_schema\"\n",
    "table = \"california_housing\"\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f\"region={region}\")\n",
    "\n",
    "env = {\"SECRET_ID\": sf_secret_id, \n",
    "       \"SF_ACCOUNT\": sf_account_id,\n",
    "       \"SF_WAREHOUSE\": warehouse,\n",
    "       \"SF_DATABASE\": database,\n",
    "       \"SF_SCHEMA\": schema,\n",
    "       \"SF_TABLE\": table,\n",
    "       \"AWS_REGION\": region}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1d776b",
   "metadata": {},
   "source": [
    "### Define Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85089374",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"max_depth\": \"5\",\n",
    "    \"eta\": \"0.2\",\n",
    "    \"gamma\": \"4\",\n",
    "    \"min_child_weight\": \"6\",\n",
    "    \"subsample\": \"0.7\",\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"num_round\": \"50\",\n",
    "    \"verbosity\": \"2\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325534c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Distributed data parallel training\n",
    "\n",
    "For distributed data parallel training we set the `instance_count > 1` and provide an qual amount of random subset of the data to each training instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82708b0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = \"ml.m5.2xlarge\"\n",
    "instance_count = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8f0ed2",
   "metadata": {},
   "source": [
    "### Launch a training job\n",
    "\n",
    "With the data uploaded and script prepared, we are ready to configure SageMaker training jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce319ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "\n",
    "role = get_execution_role()\n",
    "sm_session = sagemaker.Session()\n",
    "bucket = None #optionally specify your bucket here, eg: 'mybucket-us-east-1'; Otherwise, SageMaker will use \n",
    "              #the default acct bucket to upload model artifacts\n",
    "if bucket is None and sm_session is not None:\n",
    "    bucket = sm_session.default_bucket()\n",
    "print(f\"bucket={bucket}, role={role}\")\n",
    "prefix = \"sagemaker/sagemaker-snowflake-example\"\n",
    "output_path = \"s3://{}/{}/{}/output\".format(bucket, prefix, \"housing-dist-xgb\")\n",
    "custom_img_name = \"xgboost-ddp-training-custom\"\n",
    "custom_img_tag = \"latest\"\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "# collect default subnet IDs to deploy Sagemaker training job into\n",
    "ec2_session = boto3.Session(region_name=region)\n",
    "ec2_resource = ec2_session.resource(\"ec2\")\n",
    "subnet_ids = []\n",
    "for vpc in ec2_resource.vpcs.all():\n",
    "    # here you can choose which subnet based on the id\n",
    "    if vpc.is_default == True:\n",
    "        for subnet in vpc.subnets.all():\n",
    "            if subnet.default_for_az == True:\n",
    "                subnet_ids.append(subnet.id)\n",
    "\n",
    "# Retrieve XGBoost custom container from ECR registry path  \n",
    "xgb_image_uri = image_uris.retrieve('xgboost', region, version='1.5-1')\n",
    "print(f\"\\nusing XGBoost image: {xgb_image_uri}\")\n",
    "\n",
    "# Create Sagemaker Estimator\n",
    "xgb_script_mode_estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri = xgb_image_uri,\n",
    "    role=role,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    output_path=\"s3://{}/{}/output\".format(bucket, prefix),\n",
    "    sagemaker_session=sm_session,\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"./src\",\n",
    "    hyperparameters=hyperparams,\n",
    "    environment=env,\n",
    "    subnets = subnet_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2f6891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Estimator fitting\n",
    "xgb_script_mode_estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee9c554",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"the trained model is available in S3 -> {xgb_script_mode_estimator.model_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1439033c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Remember that the training job that we ran is very \"light\", due to the very small dataset. As such, running locally on the notebook instance results in a faster execution time, compared to SageMaker. SageMaker takes longer time to run the job because it has to provision the training infrastructure. Since this example training job not very resource-intensive, the infrastructure provisioning process adds more overhead, compared to the training job itself.\n",
    "\n",
    "In a real situation, where datasets are large, running on SageMaker can considerably speed up the execution process - and help us optimize costs, by keeping this interactive notebook environment modest and spinning up more powerful training job resources on-demand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c252de2c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleaning up\n",
    "\n",
    "To avoid incurring future charges, delete the resources. You can do this by deleting the cloud formation template used to create the IAM role and the Amazon SageMaker Notebook.\n",
    "![Cleaning Up](img/cfn-delete.png)\n",
    "\n",
    "You will have to delete the Snowflake resources manually from the Snowflake console."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0782db29",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this notebook we saw how to download data stored in Snowflake table to Sagemaker Training job instance and train a XGBoost model using a custom training container. **This approach allows us to directly integrate Snowflake as a data source with Sagemaker notebook without having the data staged on S3.**"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "c788421e88f093f8dd506f4527624e47e75432cd7217f9d7714b2cd296741f7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
