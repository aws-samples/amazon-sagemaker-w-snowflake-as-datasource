{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bdcbdd3",
   "metadata": {},
   "source": [
    "# Snowflake as Data Source for training an ML Model with Amazon Sagemaker\n",
    "**_Use of Snowflake Data Table as Data Source for training a Sagemaker Model without having Snowflake Data to stage on S3_**\n",
    "\n",
    "This notebook works well with the `conda_python3` kernel on a SageMaker Notebook `ml.t3.medium` instance.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Objective](#Objective)\n",
    "1. [Background](#Background-(Problem-Description-and-Approach))\n",
    "1. [Create a Training Script](#Create-a-training-script)\n",
    "1. [Define `Model` Hyperparameters](#Define-Model-Hyperparameters)\n",
    "1. [Launch a training job with Python SDK](#Launch-a-training-job-with-Python-SDK)\n",
    "1. [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4334f4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "This notebook illustrates how to retrieve data stored in a [Snowflake](https://www.snowflake.com/) table and use it for training an ML model using Amazon SageMaker _without having to first store the data in S3_. \n",
    "\n",
    "This example uses the [California Housing dataset (provided by Scikit-Learn)](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html) and trains a XGBoost model to predict house prices. A detailed description about the dataset can be found [here](https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html).\n",
    "\n",
    "To understand the code, you might also find it useful to refer to:\n",
    "\n",
    "- *The guide on [Use XGBoost with the SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/frameworks/xgboost/using_xgboost.html#)*\n",
    "- *Docker Registry Paths [Docker Registry Paths and Example Code](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html)*\n",
    "- *The [SageMaker reference for Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#client) (The general AWS SDK for Python, including low-level bindings for SageMaker as well as many other AWS services)*\n",
    "---\n",
    "\n",
    "## Background (Problem Description and Approach)\n",
    "\n",
    "- **Problem statement**: SageMaker requires the training data to be present either in [S3 or in EFS or in FSX for Lustre](https://aws.amazon.com/blogs/machine-learning/choose-the-best-data-source-for-your-amazon-sagemaker-training-job/). In order to train a model using data stored outside of the three supported storage services, the data first needs to be ingested into one of these services (typically S3). This requires building a data pipeline (using tools such as [Amazon SageMaker Data Wrangler](https://aws.amazon.com/sagemaker/data-wrangler/)) to move data into S3. However, this may create a data management challenge in some situations (data lifecycle management, access control etc.) and it may be desirable to have the data accessible to SageMaker _without_ the intermediate storage of data into S3. This notebook illustrates a way to do this using Snowflake as a 3rd party data source.\n",
    "\n",
    "- **Our approach**: Launch a SageMaker Training Job using the SageMaker SDK with a custom training script and have the training script download the data from Snowflake directly into the instance created for running the training job thus avoiding the temporary storage of data in S3. Use the [Snowflake Connector for Python](https://docs.snowflake.com/en/user-guide/python-connector.html) to connect and download the data from the Snowflake. Once the data is downloaded in the training instance then the training script proceeds to train an ML model using the scikit-learn SDK. **Note that it is assumed that the data is already available in Snowflake, see [`snowflake instructions`](./snowflake-instructions.md) for instructions on creating a database in Snowflake and ingesting the California Housing dataset as a table.** We create and use a custom container for SageMaker XGBoost so that we can package a script to download credentials from Snowflake into this container from the AWS Secrets Mananger. This approach fres up the data scientists from being aware about how these credentials are stored and how to retrieve. \n",
    "\n",
    "- **Our tools**: [Amazon SageMaker SDK](https://sagemaker.readthedocs.io/en/stable/), [Snowflake Connector for Python](https://docs.snowflake.com/en/user-guide/python-connector.html) and [SageMaker XGBoost Estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/xgboost/using_xgboost.html#).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb65b13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Overall Workflow\n",
    "\n",
    "The overall workflow for this notebook is shown in the diagram below.\n",
    "\n",
    "![sagemaker snowflake](img/sm-snowflake.png)\n",
    "\n",
    "Steps 1 and 2 are executed outside of this notebook. \n",
    "\n",
    "1. See [`snowflake instructions`](./snowflake-instructions.md) for instructions on creating a database in Snowflake and ingesting the California Housing dataset as a table.\n",
    "1. See [`secrets manager instructions`](./secretsmanager-instructions.md) for instructions on storing Snowflake credentials.\n",
    "\n",
    "The following sections in this notebook describe steps 3 and 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a98f89",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Create a custom container image\n",
    "\n",
    "To create a custom container image we use the SageMaker XGBoost container image as the base and then add the following:\n",
    "\n",
    "1. [Snowflake Connector for Python](https://docs.snowflake.com/en/user-guide/python-connector.html) to the image.\n",
    "1. A script to download credentials from the AWS Secrets Mananger, these credentials are used to connect to Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2255b24a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create directories for storing scripts and Dockerfile\n",
    "\"\"\"\n",
    "!mkdir src\n",
    "!mkdir scripts\n",
    "!mkdir container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948931fa",
   "metadata": {},
   "source": [
    "### Script for downloading credentials\n",
    "\n",
    "This script is written to a file locally and then packaged inside the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729a61de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile container/snowflake_credentials.py\n",
    "\n",
    "\"\"\"\n",
    "Retrieve Snowflake password for given username from AWS SecretsManager\n",
    "\"\"\"\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "def get_credentials(username: str, region_name: str) -> str:\n",
    "    \n",
    "    client = boto3.client('secretsmanager', region_name=region_name)\n",
    "    response = client.get_secret_value(SecretId= 'snowflake-sagemaker-integration')\n",
    "    secrets_credentials = json.loads(response['SecretString'])\n",
    "    sf_password = secrets_credentials[username]\n",
    "    \n",
    "    return sf_password\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61d3f28",
   "metadata": {},
   "source": [
    "### Dockerfile\n",
    "\n",
    "This is the Dockerfile used for creating the custom container used for training. Note the use of `246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-xgboost:1.5-1` as the base image for this container. We are packaging `snowflake-connector-python==2.8.3` and `snowflake_credentials.py` script we created above into this container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4f0e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile container/Dockerfile\n",
    "\n",
    "# Build an image that can be used for training in Amazon SageMaker, we use\n",
    "# the SageMaker XGBoost as the base image as it contains support for distributed\n",
    "# training.\n",
    "FROM 246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-xgboost:1.5-1\n",
    "\n",
    "MAINTAINER Amazon AI <sage-learner@amazon.com>\n",
    "\n",
    "\n",
    "RUN apt-get -y update && apt-get install -y --no-install-recommends \\\n",
    "         wget \\\n",
    "         python3-pip \\\n",
    "         python3-setuptools \\\n",
    "         nginx \\\n",
    "         ca-certificates \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "RUN ln -s /usr/bin/python3 /usr/bin/python\n",
    "RUN ln -s /usr/bin/pip3 /usr/bin/pip\n",
    "\n",
    "# Here we get snowflake-connector python package.\n",
    "# pip leaves the install caches populated which uses a \n",
    "# significant amount of space. These optimizations save a fair \n",
    "# amount of space in the image, which reduces start up time.\n",
    "RUN pip --no-cache-dir install snowflake-connector-python==2.8.3  \n",
    "\n",
    "# Include python script for retrieving Snowflake credentials \n",
    "# from AWS SecretsManager\n",
    "ADD snowflake_credentials.py /\n",
    "\n",
    "# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\n",
    "# output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\n",
    "# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\n",
    "# PATH so that the train and serve programs are found when the container is invoked.\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "ENV PYTHONDONTWRITEBYTECODE=TRUE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bc1a4d",
   "metadata": {},
   "source": [
    "### Script to push the container image to Amazon ECR\n",
    "\n",
    "We are now going to build the container image and push to our container registry i.e. Amazon ECR. This image will be used for downloading data from Snowflake, doing data preparation and finally training the ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88ddaa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile scripts/build_and_push.sh\n",
    "\n",
    "#!/usr/bin/env bash\n",
    "# This script shows how to build the Docker image and push it to ECR to be ready for use\n",
    "# by SageMaker.\n",
    "# The argument to this script are the path to the Dockerfile, the image name and tag and the aws-region\n",
    "# in which the container is to be created. This will be used as the image on the local\n",
    "# machine and combined with the account and region to form the repository name for ECR.\n",
    "\n",
    "# override the built-in echo so that we can have a nice timestamped trace\n",
    "echo () {\n",
    "    builtin echo \"$(date +'[%m-%d %H:%M:%S]'):\" \"$@\"\n",
    "}\n",
    "\n",
    "if [ \"$#\" -eq 4 ]; then\n",
    "    dlc_account_id=$(aws sts get-caller-identity | jq .Account)\n",
    "    path_to_dockerfile=$1\n",
    "    image=$2\n",
    "    tag=$3\n",
    "    region=$4\n",
    "    \n",
    "else\n",
    "    echo \"missing mandatory command line arguments, see usage...\"\n",
    "    echo \"usage: $0 </path/to/Dockerfile> $1 <image-repo> $2 <image-tag> $3 <aws-region>\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Get the account number associated with the current IAM credentials\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    exit 255\n",
    "fi\n",
    "\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${image}:${tag}\"\n",
    "echo the full image name would be ${fullname}\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --region ${region} --repository-names \"${image}\" > /dev/null 2>&1\n",
    "if [ $? -ne 0 ]; then\n",
    "    echo \"creating ECR repository : ${fullname} \"\n",
    "    aws ecr create-repository --region ${region} --repository-name \"${image}\" > /dev/null\n",
    "else\n",
    "    echo \"${image} repo already exists in ECR\"\n",
    "fi\n",
    "\n",
    "# move to path of dockerfile\n",
    "cd ${path_to_dockerfile}\n",
    "\n",
    "# get credentials to login to ECR and, build and tag the image\n",
    "# note the use of DOCKER_BUILDKIT=1, this is needed for some mount instructions in the Dockerfile\n",
    "echo \"going to start a docker build, image=${image}, using Dockerfile=${path_to_dockerfile}\"\n",
    "aws ecr get-login-password --region ${region} \\\n",
    "| docker login --username AWS --password-stdin ${account}.dkr.ecr.${region}.amazonaws.com\n",
    "DOCKER_BUILDKIT=1 docker build . -t ${image}  --build-arg dlc_account_id=${dlc_account_id} --build-arg region=${region}\n",
    "docker tag ${image} ${fullname}\n",
    "echo ${image} created\n",
    "\n",
    "# push the image to ECR\n",
    "cmd=\"aws ecr get-login-password --region ${region} | docker login --username AWS --password-stdin ${account}.dkr.ecr.${region}.amazonaws.com\"\n",
    "echo going to run \\\"${cmd}\\\" to login to ECR\n",
    "${cmd}\n",
    "\n",
    "cmd=\"docker push ${fullname}\"\n",
    "echo going to run \\\"${cmd}\\\" to push image to ecr\n",
    "${cmd}\n",
    "if [ $? -eq 0 ]; then\n",
    "    echo \"Amazon ECR URI: ${fullname}\"\n",
    "else\n",
    "    echo \"Error: Image ${fullname} build and push failed\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "echo \"all done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149bd777",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get credentials and login to ECR to pull XGBoost image from public AWS repository  \n",
    "sagemaker_xgboost_repo=\"246618743249.dkr.ecr.us-west-2.amazonaws.com\"\n",
    "! aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin $sagemaker_xgboost_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cc225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run script to build docker custom containe image and push it to ECR \n",
    "# Set region and sagemaker URI variables \n",
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "\n",
    "region = sagemaker.Session().boto_region_name\n",
    "!bash scripts/build_and_push.sh $(pwd)/container xgboost-ddp-training-custom latest $region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910747a4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Train an XGBoost Regressor using SageMaker Training Jobs\n",
    "\n",
    "Now we are ready to train our ML model using SageMaker Training Jobs. We do this in the following steps:\n",
    "\n",
    "1. Create separate Python scripts for connecting to Snowflake, querying (downloading) the data, preparing the data for ML and finally a training scripts which ties everything together.\n",
    "\n",
    "1. Provide the training script to the SageMaker SDK [Estimator](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) along with the source directory so that all the scripts we create can be provided to the training container when the training job is run using the [Estimator.fit](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#sagemaker.estimator.EstimatorBase.fit) method.\n",
    "\n",
    "**NOTE: the data from Snowflake is downloaded directly into the training container instance and at no point is it stored in S3.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85212c16",
   "metadata": {},
   "source": [
    "### Create a Snowflake connection Script\n",
    "\n",
    "Use the [Snowflake Connector for Python](https://docs.snowflake.com/en/user-guide/python-connector.html) to connect and download the data from the Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29cc8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/connection.py\n",
    "\n",
    "\"\"\"\n",
    "Establish connection with Snowflake table - HOUSING\n",
    "\"\"\"\n",
    "\n",
    "import snowflake.connector\n",
    "import snowflake_credentials\n",
    "from snowflake_credentials import get_credentials\n",
    "\n",
    "def connect(username: str, account: str, warehouse: str, database: str, schema: str, protocol: str, region: str) -> snowflake.connector.SnowflakeConnection:\n",
    "    \n",
    "    sf_user = username\n",
    "    secret_credentials = get_credentials(sf_user, region)\n",
    "    sf_password = secret_credentials\n",
    "    sf_account = account\n",
    "    sf_warehouse = warehouse\n",
    "    sf_database = database\n",
    "    sf_schema = schema\n",
    "    sf_protocol = protocol\n",
    "    \n",
    "    print(f\"sf_user={sf_user}, sf_password=****, sf_account={sf_account}, sf_warehouse={sf_warehouse}, \"\n",
    "          f\"sf_database={sf_database}, sf_schema={sf_schema}, sf_protocol={sf_protocol}\")    \n",
    "    \n",
    "    # Read to connect to snowflake\n",
    "    ctx = snowflake.connector.connect(user=sf_user,\n",
    "                                      password=sf_password,\n",
    "                                      account=sf_account,\n",
    "                                      warehouse=sf_warehouse,\n",
    "                                      database=sf_database,\n",
    "                                      schema=sf_schema,\n",
    "                                      protocol=sf_protocol)\n",
    "    \n",
    "    # Once the connection is established we read the dataset (table)\n",
    "    # into a dataframe\n",
    "    cs=ctx.cursor()\n",
    "    print(\"\\nSnowflake connection established...\")\n",
    "    \n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882ba0e3",
   "metadata": {},
   "source": [
    "### Create a Snowflake Querying Script\n",
    "\n",
    "To query data records from Snowflake database table, and store it in a dataframe.\n",
    "\n",
    "**For distributed data parallel training we download a random subset of data into each training instance. Each training instance downloads an equal amount of data which is simply `total number of rows/ total number of training hosts`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2d5833",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/query_snowflake.py\n",
    "\n",
    "\"\"\"\n",
    "Read the HOUSING table (this is the california housing dataset used by this example)\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "\n",
    "def data_pull(ctx: snowflake.connector.SnowflakeConnection, hosts: int) -> pd.DataFrame:\n",
    "    \n",
    "    # Query Snowflake HOUSING table for number of table records\n",
    "    sql_cnt = \"select count(*) from HOUSING;\"\n",
    "    df_cnt = pd.read_sql(sql_cnt, ctx)\n",
    "\n",
    "    # Retrieve the total number of table records from dataframe\n",
    "    for index, row in df_cnt.iterrows():\n",
    "        num_of_records = row.astype(int)\n",
    "        list_num_of_rec = num_of_records.tolist()\n",
    "    tot_num_records = list_num_of_rec[0]\n",
    "\n",
    "    record_percent = round(((tot_num_records/hosts)/tot_num_records)*100)\n",
    "   \n",
    "    # Query Snowflake HOUSING table\n",
    "    sql = \"select * from HOUSING sample (\"+str(record_percent)+\");\"\n",
    "    \n",
    "    # Get the dataset into Pandas\n",
    "    df = pd.read_sql(sql, ctx)\n",
    "    \n",
    "    # Prepare the data for ML\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    downloaded_row_cnt = str(len(df))\n",
    "    \n",
    "    print(f\"Number of random rows downloaded from snowflake:{downloaded_row_cnt}\")\n",
    "    \n",
    "    df['AVG_ROOMS'] = (df['TOTAL_ROOMS']/df['HOUSEHOLDS']).round(6)\n",
    "    df['AVG_BEDROOMS'] = (df['TOTAL_BEDROOMS']/df['HOUSEHOLDS']).round(6)\n",
    "    df['AVE_OCCUP'] = (df['POPULATION']/df['HOUSEHOLDS']).round(6)\n",
    "    df.drop(['OCEAN_PROXIMITY'], axis = 1, inplace=True)\n",
    "    df.drop(['TOTAL_ROOMS'], axis = 1, inplace=True)\n",
    "    df.drop(['TOTAL_BEDROOMS'], axis = 1, inplace=True)\n",
    "    df.drop(['HOUSEHOLDS'], axis = 1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0c5a91",
   "metadata": {},
   "source": [
    "### Create a Data Preparation Script\n",
    "\n",
    "The input dataframe is split into training and test datasets using [SKlearn's train_test_split function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) The split datasets are then converted to [XGB DMatrices](https://xgboost.readthedocs.io/en/stable/python/python_api.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce0e51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/data_preparation.py\n",
    "\n",
    "\"\"\"\n",
    "Preparation of training and test datasets for XGBoost Model\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prepare_data(df: pd.DataFrame) -> tuple:\n",
    "    \n",
    "    #Preparation of training and test datasets\n",
    "    X = df.drop(['MEDIAN_HOUSE_VALUE'], axis=1)  \n",
    "    y = df['MEDIAN_HOUSE_VALUE']/100000\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "    \n",
    "    print(\"building training and testing datasets\")\n",
    "    features = [\"LONGITUDE\", \"LATITUDE\", \"HOUSING_MEDIAN_AGE\", \"POPULATION\", \"MEDIAN_INCOME\", \"AVG_ROOMS\", \"AVG_BEDROOMS\", \"AVE_OCCUP\"]\n",
    "    X_train = X_train[features]\n",
    "    X_test = X_test[features]\n",
    "\n",
    "    # -- MODEL TRAINING --    \n",
    "    print(\"going to train the model\")\n",
    "    \n",
    "    # Converting input datasets to XGB DMatrices for XGBoost Model Training\n",
    "    dtrain = xgb.DMatrix(X_train.values, y_train.values)\n",
    "    dval = xgb.DMatrix(X_test.values, y_test.values)\n",
    "    watchlist = [(dtrain, 'train'), (dval, 'validation')] if dval is not None else [(dtrain, 'train')]\n",
    "    \n",
    "    return dtrain, dval, watchlist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dfc695",
   "metadata": {},
   "source": [
    "### Create a Training Script\n",
    "\n",
    "The [SageMaker Scikit-Learn Framework Container](https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-docker-containers-scikit-learn-spark.html) provides the basic runtime, and we provide a custom [`training script`](./src/train.py) that contains the actual ML training code. The folder containing the training script can optionally also contain a [`requirements.txt`](./src/requirements.txt) file to specify any additional dependencies that need to be installed in the training instance.\n",
    "\n",
    "You can find detailed guidance in the documentation on [Preparing a Scikit-Learn training script](https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/using_sklearn.html#prepare-a-scikit-learn-training-script) (for training).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bc48db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/train.py\n",
    "\n",
    "\"\"\"\n",
    "Train a Sagemaker XGBoost Model on the california housing dataset\n",
    "\"\"\"\n",
    "\n",
    "# Import dependencies\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import argparse\n",
    "import pickle as pkl\n",
    "import xgboost as xgb\n",
    "from connection import connect\n",
    "from query_snowflake import data_pull\n",
    "from data_preparation import prepare_data\n",
    "from sagemaker_containers import entry_point\n",
    "from sagemaker_xgboost_container import distributed\n",
    "from sagemaker_xgboost_container.data_utils import get_dmatrix\n",
    "\n",
    "def _xgb_train(params: dict, dtrain: xgb.DMatrix, evals: list, num_boost_round: int, model_dir: str, is_master: bool) -> None:\n",
    "    \"\"\"Run xgb train on arguments given with rabit initialized.\n",
    "\n",
    "    This is our rabit execution function.\n",
    "\n",
    "    :param args_dict: Argument dictionary used to run xgb.train().\n",
    "    :param is_master: True if current node is master host in distributed training,\n",
    "                        or is running single node training job.\n",
    "                        Note that rabit_run includes this argument.\n",
    "    \"\"\"\n",
    "    booster = xgb.train(params=params,\n",
    "                        dtrain=dtrain,\n",
    "                        evals=evals,\n",
    "                        num_boost_round=num_boost_round)\n",
    "\n",
    "    if is_master:\n",
    "        model_location = model_dir + '/xgboost-model'\n",
    "        pkl.dump(booster, open(model_location, 'wb'))\n",
    "        logging.info(\"Stored trained model at {}\".format(model_location))\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyperparameters are described here.\n",
    "    parser.add_argument('--max_depth', type=int,)\n",
    "    parser.add_argument('--eta', type=float)\n",
    "    parser.add_argument('--gamma', type=int)\n",
    "    parser.add_argument('--min_child_weight', type=int)\n",
    "    parser.add_argument('--subsample', type=float)\n",
    "    parser.add_argument('--verbosity', type=int)\n",
    "    parser.add_argument('--objective', type=str)\n",
    "    parser.add_argument('--num_round', type=int)\n",
    "    parser.add_argument('--tree_method', type=str, default=\"auto\")\n",
    "    parser.add_argument('--predictor', type=str, default=\"auto\")\n",
    "\n",
    "    # Sagemaker specific arguments. Defaults are set in the environment variables.\n",
    "    parser.add_argument('--output_data_dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--validation', type=str, default=os.environ.get('SM_CHANNEL_VALIDATION'))\n",
    "    parser.add_argument('--sm_hosts', type=str, default=os.environ.get('SM_HOSTS'))\n",
    "    parser.add_argument('--sm_current_host', type=str, default=os.environ.get('SM_CURRENT_HOST'))\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # Get SageMaker host information from runtime environment variables\n",
    "    sm_hosts = json.loads(args.sm_hosts)\n",
    "    sm_current_host = args.sm_current_host   \n",
    "    \n",
    "    # snowflake related params are read from environment variables\n",
    "    user = os.environ[\"SF_USER\"]\n",
    "    account = os.environ[\"SF_ACCOUNT\"]\n",
    "    warehouse = os.environ[\"SF_WAREHOUSE\"]\n",
    "    database = os.environ[\"SF_DATABASE\"]\n",
    "    schema = os.environ[\"SF_SCHEMA\"]\n",
    "    region = os.environ[\"AWS_REGION\"]\n",
    "    \n",
    "    protocol = \"https\"\n",
    "    \n",
    "    # Connect to Snowflake database table \n",
    "    ctx = connect(user, account, warehouse, database, schema, protocol, region)\n",
    "    \n",
    "    # Query data from Snowflake database table\n",
    "    df = data_pull(ctx, len(sm_hosts))\n",
    "    \n",
    "    # Preparation of training and test datasets\n",
    "    dtrain, dval, watchlist = prepare_data(df)\n",
    "    \n",
    "    # Define training hyperparameters\n",
    "    train_hp = {\n",
    "        'max_depth': args.max_depth,\n",
    "        'eta': args.eta,\n",
    "        'gamma': args.gamma,\n",
    "        'min_child_weight': args.min_child_weight,\n",
    "        'subsample': args.subsample,\n",
    "        'verbosity': args.verbosity,\n",
    "        'objective': args.objective,\n",
    "        'tree_method': args.tree_method,\n",
    "        'predictor': args.predictor,\n",
    "    }\n",
    "    \n",
    "    xgb_train_args = dict(\n",
    "        params=train_hp,\n",
    "        dtrain=dtrain,\n",
    "        evals=watchlist,\n",
    "        num_boost_round=args.num_round,\n",
    "        model_dir=args.model_dir)\n",
    "\n",
    "    if len(sm_hosts) > 1:\n",
    "        # Wait until all hosts are able to find each other\n",
    "        entry_point._wait_hostname_resolution()\n",
    "\n",
    "        # Execute training function after initializing rabit.\n",
    "        distributed.rabit_run(\n",
    "            exec_fun=_xgb_train,\n",
    "            args=xgb_train_args,\n",
    "            include_in_training=(dtrain is not None),\n",
    "            hosts=sm_hosts,\n",
    "            current_host=sm_current_host,\n",
    "            update_rabit_args=True\n",
    "        )\n",
    "    else:\n",
    "        # If single node training, call training method directly.\n",
    "        if dtrain:\n",
    "            xgb_train_args['is_master'] = True\n",
    "            _xgb_train(**xgb_train_args)\n",
    "        else:\n",
    "            raise ValueError(\"Training channel must have data to train model.\")\n",
    "\n",
    "\n",
    "def model_fn(model_dir: str):\n",
    "    \"\"\"Deserialize and return fitted model.\n",
    "\n",
    "    Note that this should have the same name as the serialized model in the _xgb_train method\n",
    "    \"\"\"\n",
    "    model_file = 'xgboost-model'\n",
    "    booster = pkl.load(open(os.path.join(model_dir, model_file), 'rb'))\n",
    "    return booster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd32564",
   "metadata": {},
   "source": [
    "#### We provide the Snowflake username and connection details as environment variables to the Training container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a8d500",
   "metadata": {},
   "outputs": [],
   "source": [
    "user=\"your-snowflake-user-name\"\n",
    "account=\"your-snowflake-account\"\n",
    "warehouse=\"your-snowflake-dwh\"\n",
    "database=\"your-snowflake-db\"\n",
    "schema=\"your-snowflake-schema\"\n",
    "\n",
    "env = {\"SF_USER\": user, \n",
    "       \"SF_ACCOUNT\" : account,\n",
    "       \"SF_WAREHOUSE\" : warehouse,\n",
    "       \"SF_DATABASE\": database,\n",
    "       \"SF_SCHEMA\" : schema,\n",
    "       \"AWS_REGION\": region}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1d776b",
   "metadata": {},
   "source": [
    "### Define Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85089374",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"max_depth\": \"5\",\n",
    "    \"eta\": \"0.2\",\n",
    "    \"gamma\": \"4\",\n",
    "    \"min_child_weight\": \"6\",\n",
    "    \"subsample\": \"0.7\",\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"num_round\": \"50\",\n",
    "    \"verbosity\": \"2\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325534c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Distributed data parallel training\n",
    "\n",
    "For distributed data parallel training we set the `instance_count > 1` and provide an qual amount of random subset of the data to each training instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82708b0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = \"ml.m5.2xlarge\"\n",
    "instance_count = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8f0ed2",
   "metadata": {},
   "source": [
    "### Launch a training job\n",
    "\n",
    "With the data uploaded and script prepared, we are ready to configure SageMaker training jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce319ad",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "\n",
    "session = Session()\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "role = get_execution_role()\n",
    "\n",
    "prefix = \"sagemaker/DEMO-xgboost-dist-script\"\n",
    "output_path = \"s3://{}/{}/{}/output\".format(bucket, prefix, \"housing-dist-xgb\")\n",
    "custom_img_name = \"xgboost-ddp-training-custom\"\n",
    "custom_img_tag = \"latest\"\n",
    "account_id = role.split(\":\")[4]\n",
    "\n",
    "# collect default subnet IDs\n",
    "ec2_session = boto3.Session(region_name= region)\n",
    "ec2_resource = ec2_session.resource(\"ec2\")\n",
    "subnet_ids = []\n",
    "for vpc in ec2_resource.vpcs.all():\n",
    "    # here you can choose which subnet based on the id\n",
    "    if vpc.is_default == True:\n",
    "        for subnet in vpc.subnets.all():\n",
    "            if subnet.default_for_az == True:\n",
    "                subnet_ids.append(subnet.id)\n",
    "                \n",
    "# Retrieve XGBoost custom container from ECR registry path \n",
    "custom_img_uri = account_id+\".dkr.ecr.\"+region+\".amazonaws.com/\"+custom_img_name+\":\"+custom_img_tag\n",
    "print(f\"\\nUsing Custom Image URI: {custom_img_uri}\")\n",
    "\n",
    "# Create Sagemaker Estimator\n",
    "xgb_script_mode_estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri = custom_img_uri,\n",
    "    role=role,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    output_path=\"s3://{}/{}/output\".format(bucket, prefix),\n",
    "    sagemaker_session=session,\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"./src\",\n",
    "    hyperparameters=hyperparams,\n",
    "    environment=env,\n",
    "    subnets = subnet_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2f6891",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Estimator fitting\n",
    "xgb_script_mode_estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee9c554",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"the trained model is available in S3 -> {xgb_script_mode_estimator.model_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1439033c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Remember that the training job that we ran is very \"light\", due to the very small dataset. As such, running locally on the notebook instance results in a faster execution time, compared to SageMaker. SageMaker takes longer time to run the job because it has to provision the training infrastructure. Since this example training job not very resource-intensive, the infrastructure provisioning process adds more overhead, compared to the training job itself.\n",
    "\n",
    "In a real situation, where datasets are large, running on SageMaker can considerably speed up the execution process - and help us optimize costs, by keeping this interactive notebook environment modest and spinning up more powerful training job resources on-demand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0782db29",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this notebook we saw how to download data stored in Snowflake table to Sagemaker Training job instance and train a XGBoost model using a custom training container. **This approach allows us to directly integrate Snowflake as a data source with Sagemaker notebook without having the data staged on S3.**"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
