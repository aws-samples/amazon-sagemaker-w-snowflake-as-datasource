{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bdcbdd3",
   "metadata": {},
   "source": [
    "# Snowflake as Data Source for training an ML Model with Amazon Sagemaker\n",
    "**_Use of Snowflake Data Table as Data Source for training a Sagemaker Model without having Snowflake Data to stage on S3_**\n",
    "\n",
    "This notebook works well with the `conda_python3` kernel on a SageMaker Notebook `ml.t3.xlarge` instance.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Objective](#Objective)\n",
    "1. [Background](#Background-(Problem-Description-and-Approach))\n",
    "1. [Create a Training Script](#Create-a-training-script)\n",
    "1. [Define `Model` Hyperparameters](#Define-Model-Hyperparameters)\n",
    "1. [Launch a training job with Python SDK](#Launch-a-training-job-with-Python-SDK)\n",
    "1. [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4334f4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "This notebook illustrates how to retrieve data stored in a [Snowflake](https://www.snowflake.com/) table and use it for training an ML model using Amazon SageMaker _without having to first store the data in S3_. \n",
    "\n",
    "This example uses the [California Housing dataset (provided by Scikit-Learn)](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html) and trains a XGBoost model to predict house prices. A detailed description about the dataset can be found [here](https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html).\n",
    "\n",
    "To understand the code, you might also find it useful to refer to:\n",
    "\n",
    "- *The guide on [Use XGBoost with the SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/frameworks/xgboost/using_xgboost.html#)*\n",
    "- *Docker Registry Paths [Docker Registry Paths and Example Code](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html)*\n",
    "- *The [SageMaker reference for Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#client) (The general AWS SDK for Python, including low-level bindings for SageMaker as well as many other AWS services)*\n",
    "---\n",
    "\n",
    "## Background (Problem Description and Approach)\n",
    "\n",
    "- **Problem statement**: SageMaker requires the training data to be present either in [S3 or in EFS or in FSX for Lustre](https://aws.amazon.com/blogs/machine-learning/choose-the-best-data-source-for-your-amazon-sagemaker-training-job/). In order to train a model using data stored outside of the three supported storage services, the data first needs to be ingested into one of these services (typically S3). This requires building a data pipeline (using tools such as [Amazon SageMaker Data Wrangler](https://aws.amazon.com/sagemaker/data-wrangler/)) to move data into S3. However, this may create a data management challenge in some situations (data lifecycle management, access control etc.) and it may be desirable to have the data accessible to SageMaker _without_ the intermediate storage of data into S3. This notebook illustrates a way to do this using Snowflake as a 3rd party data source.\n",
    "\n",
    "- **Our approach**: Launch a SageMaker Training Job using the SageMaker SDK with a custom training script and have the training script download the data from Snowflake directly into the instance created for running the training job thus avoiding the temporary storage of data in S3. Use the [Snowflake Connector for Python](https://docs.snowflake.com/en/user-guide/python-connector.html) to connect and download the data from the Snowflake. Once the data is downloaded in the training instance then the training script proceeds to train an ML model using the scikit-learn SDK. **Note that it is assumed that the data is already available in Snowflake, see [`snowflake instructions`](./snowflake-instructions.md) for instructions on creating a database in Snowflake and ingesting the California Housing dataset as a table.** We create and use a custom container for SageMaker XGBoost so that we can package a script to download credentials from Snowflake into this container from the AWS Secrets Mananger. This approach fres up the data scientists from being aware about how these credentials are stored and how to retrieve. \n",
    "\n",
    "- **Our tools**: [Amazon SageMaker SDK](https://sagemaker.readthedocs.io/en/stable/), [Snowflake Connector for Python](https://docs.snowflake.com/en/user-guide/python-connector.html) and [SageMaker XGBoost Estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/xgboost/using_xgboost.html#).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb65b13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Overall Workflow\n",
    "\n",
    "The overall workflow for this notebook is shown in the diagram below.\n",
    "\n",
    "![sagemaker snowflake](img/sm-snowflake.png)\n",
    "\n",
    "Steps 1 and 2 are executed outside of this notebook. \n",
    "\n",
    "1. See [`snowflake instructions`](./snowflake-instructions.md) for instructions on creating a database in Snowflake and ingesting the California Housing dataset as a table.\n",
    "1. See [`secrets manager instructions`](./secretsmanager-instructions.md) for instructions on storing Snowflake credentials.\n",
    "\n",
    "The following sections in this notebook describe steps 3 and 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a98f89",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Create a custom container image\n",
    "\n",
    "To create a custom container image we use the SageMaker XGBoost container image as the base and then add the following:\n",
    "\n",
    "1. [Snowflake Connector for Python](https://docs.snowflake.com/en/user-guide/python-connector.html) to the image.\n",
    "1. A script to download credentials from the AWS Secrets Mananger, these credentials are used to connect to Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8740532d-02f5-4273-9b02-9128fb32764e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: boto3==1.26.44 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.26.44)\n",
      "Requirement already satisfied: sagemaker==2.127.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.127.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3==1.26.44) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.44 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3==1.26.44) (1.29.55)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3==1.26.44) (0.10.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.127.0) (1.0.1)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.127.0) (1.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.127.0) (21.3)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.127.0) (0.3.0)\n",
      "Requirement already satisfied: attrs<23,>=20.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.127.0) (22.1.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.127.0) (1.22.4)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.127.0) (3.20.3)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.127.0) (0.7.5)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.127.0) (4.13.0)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.127.0) (0.2.0)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.127.0) (0.1.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.44->boto3==1.26.44) (1.26.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.44->boto3==1.26.44) (2.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker==2.127.0) (3.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.0->sagemaker==2.127.0) (3.0.9)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker==2.127.0) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker==2.127.0) (2022.5)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker==2.127.0) (1.7.6.6)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker==2.127.0) (0.70.14)\n",
      "Requirement already satisfied: dill>=0.3.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker==2.127.0) (0.3.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker==2.127.0) (0.3.2)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from schema->sagemaker==2.127.0) (21.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3==1.26.44 sagemaker==2.127.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2255b24a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘src’: File exists\n",
      "mkdir: cannot create directory ‘scripts’: File exists\n",
      "mkdir: cannot create directory ‘container’: File exists\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create directories for storing scripts and Dockerfile\n",
    "\"\"\"\n",
    "!mkdir src\n",
    "!mkdir scripts\n",
    "!mkdir container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948931fa",
   "metadata": {},
   "source": [
    "### Script for downloading credentials\n",
    "\n",
    "This script is written to a file locally and then packaged inside the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "729a61de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting container/snowflake_credentials.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile container/snowflake_credentials.py\n",
    "\n",
    "\"\"\"\n",
    "Retrieve Snowflake password for given username from AWS SecretsManager\n",
    "\"\"\"\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "def get_credentials(secret_id: str, region_name: str) -> str:\n",
    "    \n",
    "    client = boto3.client('secretsmanager', region_name=region_name)\n",
    "    response = client.get_secret_value(SecretId=secret_id)\n",
    "    secrets_value = json.loads(response['SecretString'])    \n",
    "    \n",
    "    return secrets_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61d3f28",
   "metadata": {},
   "source": [
    "### Dockerfile\n",
    "\n",
    "This is the Dockerfile used for creating the custom container used for training. Note the use of `246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-xgboost:1.5-1` as the base image for this container. We are packaging `snowflake-connector-python==2.8.3` and `snowflake_credentials.py` script we created above into this container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd4f0e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting container/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile container/Dockerfile\n",
    "\n",
    "# Build an image that can be used for training in Amazon SageMaker, we use\n",
    "# the SageMaker XGBoost as the base image as it contains support for distributed\n",
    "# training.\n",
    "FROM 246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-xgboost:1.5-1\n",
    "\n",
    "RUN apt-get -y update && apt-get install -y --no-install-recommends \\\n",
    "         wget \\\n",
    "         python3-pip \\\n",
    "         python3-setuptools \\\n",
    "         nginx \\\n",
    "         ca-certificates \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "RUN ln -s /usr/bin/python3 /usr/bin/python\n",
    "RUN ln -s /usr/bin/pip3 /usr/bin/pip\n",
    "\n",
    "# Here we get snowflake-connector python package.\n",
    "# pip leaves the install caches populated which uses a \n",
    "# significant amount of space. These optimizations save a fair \n",
    "# amount of space in the image, which reduces start up time.\n",
    "#RUN pip --no-cache-dir install snowflake-connector-python==2.8.3\n",
    "RUN pip --no-cache-dir install snowflake-connector-python==2.9.0\n",
    "\n",
    "# Include python script for retrieving Snowflake credentials \n",
    "# from AWS SecretsManager\n",
    "ADD snowflake_credentials.py /\n",
    "\n",
    "# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\n",
    "# output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\n",
    "# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\n",
    "# PATH so that the train and serve programs are found when the container is invoked.\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "ENV PYTHONDONTWRITEBYTECODE=TRUE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bc1a4d",
   "metadata": {},
   "source": [
    "### Script to push the container image to Amazon ECR\n",
    "\n",
    "We are now going to build the container image and push to our container registry i.e. Amazon ECR. This image will be used for downloading data from Snowflake, doing data preparation and finally training the ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b88ddaa4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/build_and_push.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/build_and_push.sh\n",
    "\n",
    "#!/usr/bin/env bash\n",
    "# This script shows how to build the Docker image and push it to ECR to be ready for use\n",
    "# by SageMaker.\n",
    "# The argument to this script are the path to the Dockerfile, the image name and tag and the aws-region\n",
    "# in which the container is to be created. This will be used as the image on the local\n",
    "# machine and combined with the account and region to form the repository name for ECR.\n",
    "\n",
    "# override the built-in echo so that we can have a nice timestamped trace\n",
    "echo () {\n",
    "    builtin echo \"$(date +'[%m-%d %H:%M:%S]'):\" \"$@\"\n",
    "}\n",
    "\n",
    "if [ \"$#\" -eq 4 ]; then\n",
    "    dlc_account_id=$(aws sts get-caller-identity | jq .Account)\n",
    "    path_to_dockerfile=$1\n",
    "    image=$2\n",
    "    tag=$3\n",
    "    region=$4\n",
    "    \n",
    "else\n",
    "    echo \"missing mandatory command line arguments, see usage...\"\n",
    "    echo \"usage: $0 </path/to/Dockerfile> $1 <image-repo> $2 <image-tag> $3 <aws-region>\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Get the account number associated with the current IAM credentials\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    exit 255\n",
    "fi\n",
    "\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${image}:${tag}\"\n",
    "echo the full image name would be ${fullname}\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --region ${region} --repository-names \"${image}\" > /dev/null 2>&1\n",
    "if [ $? -ne 0 ]; then\n",
    "    echo \"creating ECR repository : ${fullname} \"\n",
    "    aws ecr create-repository --region ${region} --repository-name \"${image}\" > /dev/null\n",
    "else\n",
    "    echo \"${image} repo already exists in ECR\"\n",
    "fi\n",
    "\n",
    "# move to path of dockerfile\n",
    "cd ${path_to_dockerfile}\n",
    "\n",
    "# get credentials to login to ECR and, build and tag the image\n",
    "# note the use of DOCKER_BUILDKIT=1, this is needed for some mount instructions in the Dockerfile\n",
    "echo \"going to start a docker build, image=${image}, using Dockerfile=${path_to_dockerfile}\"\n",
    "aws ecr get-login-password --region ${region} \\\n",
    "| docker login --username AWS --password-stdin ${account}.dkr.ecr.${region}.amazonaws.com\n",
    "DOCKER_BUILDKIT=1 docker build . -t ${image}  --build-arg dlc_account_id=${dlc_account_id} --build-arg region=${region}\n",
    "docker tag ${image} ${fullname}\n",
    "echo ${image} created\n",
    "\n",
    "# push the image to ECR\n",
    "cmd=\"aws ecr get-login-password --region ${region} | docker login --username AWS --password-stdin ${account}.dkr.ecr.${region}.amazonaws.com\"\n",
    "echo going to run \\\"${cmd}\\\" to login to ECR\n",
    "${cmd}\n",
    "\n",
    "cmd=\"docker push ${fullname}\"\n",
    "echo going to run \\\"${cmd}\\\" to push image to ecr\n",
    "${cmd}\n",
    "if [ $? -eq 0 ]; then\n",
    "    echo \"Amazon ECR URI: ${fullname}\"\n",
    "else\n",
    "    echo \"Error: Image ${fullname} build and push failed\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "echo \"all done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "149bd777",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region=us-east-1\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "# Get credentials and login to ECR to pull XGBoost image from public AWS repository  \n",
    "import boto3\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f\"region={region}\")\n",
    "sagemaker_xgboost_repo=\"246618743249.dkr.ecr.us-west-2.amazonaws.com\"\n",
    "! aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin $sagemaker_xgboost_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5cc225f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01-24 06:36:53]: the full image name would be 328296961357.dkr.ecr.us-east-1.amazonaws.com/xgboost-ddp-training-custom:latest\n",
      "[01-24 06:36:54]: xgboost-ddp-training-custom repo already exists in ECR\n",
      "[01-24 06:36:54]: going to start a docker build, image=xgboost-ddp-training-custom, using Dockerfile=/home/ec2-user/SageMaker/snowflake-sermolin-review-2/container\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                                         \n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.1s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.50kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for 246618743249.dkr.ecr.us-west-2.amazonaws  0.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.50kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for 246618743249.dkr.ecr.us-west-2.amazonaws  0.2s\n",
      "\u001b[34m => [auth] sharing credentials for 246618743249.dkr.ecr.us-west-2.amazona  0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.4s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.50kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for 246618743249.dkr.ecr.us-west-2.amazonaws  0.3s\n",
      "\u001b[34m => [auth] sharing credentials for 246618743249.dkr.ecr.us-west-2.amazona  0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.5s (5/11)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.50kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for 246618743249.dkr.ecr.us-west-2.amazonaws  0.4s\n",
      "\u001b[0m\u001b[34m => [auth] sharing credentials for 246618743249.dkr.ecr.us-west-2.amazona  0.0s\n",
      "\u001b[0m\u001b[34m => [1/6] FROM 246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-xgb  0.0s\n",
      "\u001b[0m => [internal] load build context                                          0.0s\n",
      "\u001b[34m => => transferring context: 445B                                          0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.6s (12/12) FINISHED                                              \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.50kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for 246618743249.dkr.ecr.us-west-2.amazonaws  0.4s\n",
      "\u001b[0m\u001b[34m => [auth] sharing credentials for 246618743249.dkr.ecr.us-west-2.amazona  0.0s\n",
      "\u001b[0m\u001b[34m => [1/6] FROM 246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-xgb  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 445B                                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/6] RUN apt-get -y update && apt-get install -y --no-install  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/6] RUN ln -s /usr/bin/python3 /usr/bin/python                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/6] RUN ln -s /usr/bin/pip3 /usr/bin/pip                      0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [5/6] RUN pip --no-cache-dir install snowflake-connector-pytho  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [6/6] ADD snowflake_credentials.py /                            0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.1s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:99804b16e407121e240aa7077cd6c3e456fa4e3c4dcd7  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/xgboost-ddp-training-custom             0.0s\n",
      "\u001b[0m\u001b[?25h[01-24 06:36:55]: xgboost-ddp-training-custom created\n",
      "[01-24 06:36:55]: going to run \"aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 328296961357.dkr.ecr.us-east-1.amazonaws.com\" to login to ECR\n",
      "\n",
      "Unknown options: |,docker,login,--username,AWS,--password-stdin,328296961357.dkr.ecr.us-east-1.amazonaws.com\n",
      "[01-24 06:36:56]: going to run \"docker push 328296961357.dkr.ecr.us-east-1.amazonaws.com/xgboost-ddp-training-custom:latest\" to push image to ecr\n",
      "The push refers to repository [328296961357.dkr.ecr.us-east-1.amazonaws.com/xgboost-ddp-training-custom]\n",
      "\n",
      "\u001b[1B883af6c2: Preparing \n",
      "\u001b[1Bf50b56e5: Preparing \n",
      "\u001b[1B0802c64b: Preparing \n",
      "\u001b[1Ba32a4d71: Preparing \n",
      "\u001b[1B08a1e529: Preparing \n",
      "\u001b[1B2d73fc7c: Preparing \n",
      "\u001b[1B7b1b841a: Preparing \n",
      "\u001b[1B938af681: Preparing \n",
      "\u001b[4B2d73fc7c: Waiting g \n",
      "\u001b[4B7b1b841a: Waiting g \n",
      "\u001b[1B2a794261: Preparing \n",
      "\u001b[1Bfcfafa43: Preparing \n",
      "\u001b[1Ba9a880e9: Preparing \n",
      "\u001b[1B1be04191: Preparing \n",
      "\u001b[1B5f95f696: Preparing \n",
      "\u001b[1B686af23f: Preparing \n",
      "\u001b[1B4f98f32a: Preparing \n",
      "\u001b[11B38af681: Waiting g \n",
      "\u001b[1B645d99d8: Preparing \n",
      "\u001b[12B69e1280: Waiting g \n",
      "\u001b[12B91106d7: Waiting g \n",
      "\u001b[1B95ca8e81: Preparing \n",
      "\u001b[1B73ca137f: Preparing \n",
      "\u001b[14Ba794261: Waiting g \n",
      "\u001b[14Bcfafa43: Waiting g \n",
      "\u001b[1B1e21953d: Layer already exists \u001b[21A\u001b[2K\u001b[16A\u001b[2K\u001b[11A\u001b[2K\u001b[5A\u001b[2Klatest: digest: sha256:edf8143ecb7b02dc15ff8cc256410480209f0ed5be75f5cfa8269be4ffeb749c size: 5764\n",
      "[01-24 06:36:56]: Amazon ECR URI: 328296961357.dkr.ecr.us-east-1.amazonaws.com/xgboost-ddp-training-custom:latest\n",
      "[01-24 06:36:56]: all done\n"
     ]
    }
   ],
   "source": [
    "# Run script to build docker custom containe image and push it to ECR \n",
    "# Set region and sagemaker URI variables \n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "\n",
    "!bash scripts/build_and_push.sh $(pwd)/container xgboost-ddp-training-custom latest $region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910747a4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Train an XGBoost Regressor using SageMaker Training Jobs\n",
    "\n",
    "Now we are ready to train our ML model using SageMaker Training Jobs. We do this in the following steps:\n",
    "\n",
    "1. Create separate Python scripts for connecting to Snowflake, querying (downloading) the data, preparing the data for ML and finally a training scripts which ties everything together.\n",
    "\n",
    "1. Provide the training script to the SageMaker SDK [Estimator](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) along with the source directory so that all the scripts we create can be provided to the training container when the training job is run using the [Estimator.fit](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#sagemaker.estimator.EstimatorBase.fit) method.\n",
    "\n",
    "**NOTE: the data from Snowflake is downloaded directly into the training container instance and at no point is it stored in S3.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85212c16",
   "metadata": {},
   "source": [
    "### Create a Snowflake connection Script\n",
    "\n",
    "Use the [Snowflake Connector for Python](https://docs.snowflake.com/en/user-guide/python-connector.html) to connect and download the data from the Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e29cc8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/connection.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/connection.py\n",
    "\n",
    "\"\"\"\n",
    "Establish connection with Snowflake table - HOUSING\n",
    "\"\"\"\n",
    "\n",
    "import snowflake.connector\n",
    "import snowflake_credentials\n",
    "from snowflake_credentials import get_credentials\n",
    "\n",
    "def connect(secret_id: str, account: str, warehouse: str, database: str, schema: str, protocol: str, region: str) -> snowflake.connector.SnowflakeConnection:\n",
    "    \n",
    "    secret_value = get_credentials(secret_id, region)\n",
    "    sf_user = secret_value['username']\n",
    "    sf_password = secret_value['password']\n",
    "    sf_account = account\n",
    "    sf_warehouse = warehouse\n",
    "    sf_database = database\n",
    "    sf_schema = schema\n",
    "    sf_protocol = protocol\n",
    "    \n",
    "    print(f\"sf_user={sf_user}, sf_password=****, sf_account={sf_account}, sf_warehouse={sf_warehouse}, \"\n",
    "          f\"sf_database={sf_database}, sf_schema={sf_schema}, sf_protocol={sf_protocol}\")    \n",
    "    \n",
    "    # Read to connect to snowflake\n",
    "    ctx = snowflake.connector.connect(user=sf_user,\n",
    "                                      password=sf_password,\n",
    "                                      account=sf_account,\n",
    "                                      warehouse=sf_warehouse,\n",
    "                                      database=sf_database,\n",
    "                                      schema=sf_schema,\n",
    "                                      protocol=sf_protocol)\n",
    "    \n",
    "    # Once the connection is established we read the dataset (table)\n",
    "    # into a dataframe\n",
    "    cs=ctx.cursor()\n",
    "    print(\"\\nSnowflake connection established...\")\n",
    "    \n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882ba0e3",
   "metadata": {},
   "source": [
    "### Create a Snowflake Querying Script\n",
    "\n",
    "To query data records from Snowflake database table, and store it in a dataframe.\n",
    "\n",
    "**For distributed data parallel training we download a random subset of data into each training instance. Each training instance downloads an equal amount of data which is simply `total number of rows/ total number of training hosts`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b2d5833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/query_snowflake.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/query_snowflake.py\n",
    "\n",
    "\"\"\"\n",
    "Read the HOUSING table (this is the california housing dataset used by this example)\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "\n",
    "def data_pull(ctx: snowflake.connector.SnowflakeConnection, table: str, hosts: int) -> pd.DataFrame:\n",
    "    \n",
    "    # Query Snowflake HOUSING table for number of table records\n",
    "    sql_cnt = f\"select count(*) from {table};\"\n",
    "    df_cnt = pd.read_sql(sql_cnt, ctx)\n",
    "\n",
    "    # Retrieve the total number of table records from dataframe\n",
    "    for index, row in df_cnt.iterrows():\n",
    "        num_of_records = row.astype(int)\n",
    "        list_num_of_rec = num_of_records.tolist()\n",
    "    tot_num_records = list_num_of_rec[0]\n",
    "\n",
    "    record_percent = str(round(100/hosts))\n",
    "    print(f\"going to download a random {record_percent}% sample of the data\")\n",
    "    # Query Snowflake HOUSING table\n",
    "    sql = f\"select * from {table} sample ({record_percent});\"\n",
    "    print(f\"sql={sql}\")\n",
    "    \n",
    "    # Get the dataset into Pandas\n",
    "    df = pd.read_sql(sql, ctx)\n",
    "    print(f\"read data into a dataframe of shape {df.shape}\")\n",
    "    # Prepare the data for ML\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    print(f\"Number of rows after dropping NA: {df.shape[0]}\")\n",
    "    \"\"\"\n",
    "    df['AVG_ROOMS'] = (df['TOTAL_ROOMS']/df['HOUSEHOLDS']).round(6)\n",
    "    df['AVG_BEDROOMS'] = (df['TOTAL_BEDROOMS']/df['HOUSEHOLDS']).round(6)\n",
    "    df['AVE_OCCUP'] = (df['POPULATION']/df['HOUSEHOLDS']).round(6)\n",
    "    df.drop(['OCEAN_PROXIMITY'], axis = 1, inplace=True)\n",
    "    df.drop(['TOTAL_ROOMS'], axis = 1, inplace=True)\n",
    "    df.drop(['TOTAL_BEDROOMS'], axis = 1, inplace=True)\n",
    "    df.drop(['HOUSEHOLDS'], axis = 1, inplace=True)\n",
    "    \"\"\"\n",
    "    print(f\"final shape of dataframe to be used for training {df.shape}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0c5a91",
   "metadata": {},
   "source": [
    "### Create a Data Preparation Script\n",
    "\n",
    "The input dataframe is split into training and test datasets using [SKlearn's train_test_split function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) The split datasets are then converted to [XGB DMatrices](https://xgboost.readthedocs.io/en/stable/python/python_api.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ce0e51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/data_preparation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/data_preparation.py\n",
    "\n",
    "\"\"\"\n",
    "Preparation of training and test datasets for XGBoost Model\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prepare_data(df: pd.DataFrame) -> tuple:\n",
    "    \n",
    "    # preparation of training and test datasets\n",
    "    X = df.drop(['MEDHOUSEVAL'], axis=1)  \n",
    "    y = df['MEDHOUSEVAL']\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "    \n",
    "    print(\"building training and testing datasets\")\n",
    "    features = X.select_dtypes('number').columns.tolist()\n",
    "    print(f\"features={features}\")\n",
    "    X_train = X_train[features]\n",
    "    X_test = X_test[features]\n",
    "\n",
    "    # -- MODEL TRAINING --    \n",
    "    print(\"going to train the model\")\n",
    "    \n",
    "    # Converting input datasets to XGB DMatrices for XGBoost Model Training\n",
    "    dtrain = xgb.DMatrix(X_train.values, y_train.values)\n",
    "    dval = xgb.DMatrix(X_test.values, y_test.values)\n",
    "    watchlist = [(dtrain, 'train'), (dval, 'validation')] if dval is not None else [(dtrain, 'train')]\n",
    "    \n",
    "    return dtrain, dval, watchlist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dfc695",
   "metadata": {},
   "source": [
    "### Create a Training Script\n",
    "\n",
    "The [SageMaker Scikit-Learn Framework Container](https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-docker-containers-scikit-learn-spark.html) provides the basic runtime, and we provide a custom [`training script`](./src/train.py) that contains the actual ML training code. The folder containing the training script can optionally also contain a [`requirements.txt`](./src/requirements.txt) file to specify any additional dependencies that need to be installed in the training instance.\n",
    "\n",
    "You can find detailed guidance in the documentation on [Preparing a Scikit-Learn training script](https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/using_sklearn.html#prepare-a-scikit-learn-training-script) (for training).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20bc48db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/train.py\n",
    "\n",
    "\"\"\"\n",
    "Train a Sagemaker XGBoost Model on the california housing dataset\n",
    "\"\"\"\n",
    "\n",
    "# Import dependencies\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import argparse\n",
    "import pickle as pkl\n",
    "import xgboost as xgb\n",
    "from connection import connect\n",
    "from query_snowflake import data_pull\n",
    "from data_preparation import prepare_data\n",
    "from sagemaker_containers import entry_point\n",
    "from sagemaker_xgboost_container import distributed\n",
    "from sagemaker_xgboost_container.data_utils import get_dmatrix\n",
    "\n",
    "def _xgb_train(params: dict, dtrain: xgb.DMatrix, evals: list, num_boost_round: int, model_dir: str, is_master: bool) -> None:\n",
    "    \"\"\"Run xgb train on arguments given with rabit initialized.\n",
    "\n",
    "    This is our rabit execution function.\n",
    "\n",
    "    :param args_dict: Argument dictionary used to run xgb.train().\n",
    "    :param is_master: True if current node is master host in distributed training,\n",
    "                        or is running single node training job.\n",
    "                        Note that rabit_run includes this argument.\n",
    "    \"\"\"\n",
    "    booster = xgb.train(params=params,\n",
    "                        dtrain=dtrain,\n",
    "                        evals=evals,\n",
    "                        num_boost_round=num_boost_round)\n",
    "\n",
    "    if is_master:\n",
    "        model_location = model_dir + '/xgboost-model'\n",
    "        pkl.dump(booster, open(model_location, 'wb'))\n",
    "        logging.info(\"Stored trained model at {}\".format(model_location))\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyperparameters are described here.\n",
    "    parser.add_argument('--max_depth', type=int,)\n",
    "    parser.add_argument('--eta', type=float)\n",
    "    parser.add_argument('--gamma', type=int)\n",
    "    parser.add_argument('--min_child_weight', type=int)\n",
    "    parser.add_argument('--subsample', type=float)\n",
    "    parser.add_argument('--verbosity', type=int)\n",
    "    parser.add_argument('--objective', type=str)\n",
    "    parser.add_argument('--num_round', type=int)\n",
    "    parser.add_argument('--tree_method', type=str, default=\"auto\")\n",
    "    parser.add_argument('--predictor', type=str, default=\"auto\")\n",
    "\n",
    "    # Sagemaker specific arguments. Defaults are set in the environment variables.\n",
    "    parser.add_argument('--output_data_dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--validation', type=str, default=os.environ.get('SM_CHANNEL_VALIDATION'))\n",
    "    parser.add_argument('--sm_hosts', type=str, default=os.environ.get('SM_HOSTS'))\n",
    "    parser.add_argument('--sm_current_host', type=str, default=os.environ.get('SM_CURRENT_HOST'))\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # Get SageMaker host information from runtime environment variables\n",
    "    sm_hosts = json.loads(args.sm_hosts)\n",
    "    sm_current_host = args.sm_current_host   \n",
    "    \n",
    "    # snowflake related params are read from environment variables\n",
    "    secret_id = os.environ[\"SECRET_ID\"]\n",
    "    account = os.environ[\"SF_ACCOUNT\"]\n",
    "    warehouse = os.environ[\"SF_WAREHOUSE\"]\n",
    "    database = os.environ[\"SF_DATABASE\"].upper()\n",
    "    schema = os.environ[\"SF_SCHEMA\"].upper()\n",
    "    table = os.environ['SF_TABLE'].upper()\n",
    "    region = os.environ[\"AWS_REGION\"]\n",
    "    \n",
    "    protocol = \"https\"\n",
    "    \n",
    "    # Connect to Snowflake database table \n",
    "    ctx = connect(secret_id, account, warehouse, database, schema, protocol, region)\n",
    "    \n",
    "    # Query data from Snowflake database table\n",
    "    # IMPORTANT: DATA FROM SNOWFLAKE GOES DIRECTLY INTO PANDA DF. THE DATA DOES __NOT__ GET STAGED IN AND S3 BUCKET. \n",
    "    df = data_pull(ctx, table, len(sm_hosts))\n",
    "    \n",
    "    # Preparation of training and test datasets\n",
    "    dtrain, dval, watchlist = prepare_data(df)\n",
    "    \n",
    "    # Define training hyperparameters\n",
    "    train_hp = {\n",
    "        'max_depth': args.max_depth,\n",
    "        'eta': args.eta,\n",
    "        'gamma': args.gamma,\n",
    "        'min_child_weight': args.min_child_weight,\n",
    "        'subsample': args.subsample,\n",
    "        'verbosity': args.verbosity,\n",
    "        'objective': args.objective,\n",
    "        'tree_method': args.tree_method,\n",
    "        'predictor': args.predictor,\n",
    "    }\n",
    "    \n",
    "    xgb_train_args = dict(\n",
    "        params=train_hp,\n",
    "        dtrain=dtrain,\n",
    "        evals=watchlist,\n",
    "        num_boost_round=args.num_round,\n",
    "        model_dir=args.model_dir)\n",
    "\n",
    "    if len(sm_hosts) > 1:\n",
    "        # Wait until all hosts are able to find each other\n",
    "        entry_point._wait_hostname_resolution()\n",
    "\n",
    "        # Execute training function after initializing rabit.\n",
    "        distributed.rabit_run(\n",
    "            exec_fun=_xgb_train,\n",
    "            args=xgb_train_args,\n",
    "            include_in_training=(dtrain is not None),\n",
    "            hosts=sm_hosts,\n",
    "            current_host=sm_current_host,\n",
    "            update_rabit_args=True\n",
    "        )\n",
    "    else:\n",
    "        # If single node training, call training method directly.\n",
    "        if dtrain:\n",
    "            xgb_train_args['is_master'] = True\n",
    "            _xgb_train(**xgb_train_args)\n",
    "        else:\n",
    "            raise ValueError(\"Training channel must have data to train model.\")\n",
    "\n",
    "\n",
    "def model_fn(model_dir: str):\n",
    "    \"\"\"Deserialize and return fitted model.\n",
    "\n",
    "    Note that this should have the same name as the serialized model in the _xgb_train method\n",
    "    \"\"\"\n",
    "    model_file = 'xgboost-model'\n",
    "    booster = pkl.load(open(os.path.join(model_dir, model_file), 'rb'))\n",
    "    return booster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd32564",
   "metadata": {},
   "source": [
    "#### Provide the Snowflake username and connection details as environment variables to the Training container\n",
    "\n",
    "Retrieve the Snowflake credentials from AWS Secrets Mananger. This is done by the training code and we just need to provide the secrets identifier as an environment variable.\n",
    "\n",
    "We also need to retrieve your account identifier for Snowflake which we already obtained when we run the previous notebook (snowflake-load-dataset.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37a8d500",
   "metadata": {},
   "outputs": [],
   "source": [
    "#secret_id = \"snowflake_credentials\"\n",
    "#account = \"your-snowflake-account-id\"\n",
    "\n",
    "%store -r sf_account_id \n",
    "%store -r snowflake_secret \n",
    "\n",
    "secret_id=snowflake_secret\n",
    "account=sf_account_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "003a94e9-6e3f-4d69-a189-b04aa109a29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uub56114.us-east-1 snowflake-credentials\n",
      "snowflake-credentials uub56114.us-east-1\n"
     ]
    }
   ],
   "source": [
    "print (sf_account_id, snowflake_secret)\n",
    "print (secret_id, account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "809447f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO - replace with -r magic, make naming consistent. \n",
    "# do not change!!!\n",
    "# the values of these variables match what we put in the snowflake-load-dataset.ipynb file\n",
    "warehouse = \"amazon_sagemake_w_snowflake_as_datasource\"\n",
    "database = \"housing\"\n",
    "schema = \"housing_schema\"\n",
    "table = \"california_housing\"\n",
    "\n",
    "env = {\"SECRET_ID\": secret_id, \n",
    "       \"SF_ACCOUNT\": account,\n",
    "       \"SF_WAREHOUSE\": warehouse,\n",
    "       \"SF_DATABASE\": database,\n",
    "       \"SF_SCHEMA\": schema,\n",
    "       \"SF_TABLE\": table,\n",
    "       \"AWS_REGION\": region}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1d776b",
   "metadata": {},
   "source": [
    "### Define Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85089374",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"max_depth\": \"5\",\n",
    "    \"eta\": \"0.2\",\n",
    "    \"gamma\": \"4\",\n",
    "    \"min_child_weight\": \"6\",\n",
    "    \"subsample\": \"0.7\",\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"num_round\": \"50\",\n",
    "    \"verbosity\": \"2\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325534c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Distributed data parallel training\n",
    "\n",
    "For distributed data parallel training we set the `instance_count > 1` and provide an qual amount of random subset of the data to each training instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "82708b0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = \"ml.m5.2xlarge\"\n",
    "instance_count = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8f0ed2",
   "metadata": {},
   "source": [
    "### Launch a training job\n",
    "\n",
    "With the data uploaded and script prepared, we are ready to configure SageMaker training jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ce319ad",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket=sagemaker-us-east-1-328296961357, role=arn:aws:iam::328296961357:role/SageMakerSnowFlakeExampleIAMRole-03\n",
      "\n",
      "using custom image: 328296961357.dkr.ecr.us-east-1.amazonaws.com/xgboost-ddp-training-custom:latest\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "\n",
    "role = get_execution_role()\n",
    "sm_session = sagemaker.Session()\n",
    "bucket = None #optionally specify you bucket here, eg: 'mybucket-us-east-1'; Otherwise, SageMaker will use default acct bucket\n",
    "if bucket is None and sm_session is not None:\n",
    "    bucket = sm_session.default_bucket()\n",
    "print(f\"bucket={bucket}, role={role}\")\n",
    "prefix = \"sagemaker/sagemaker-snowflake-example\"\n",
    "output_path = \"s3://{}/{}/{}/output\".format(bucket, prefix, \"housing-dist-xgb\")\n",
    "custom_img_name = \"xgboost-ddp-training-custom\"\n",
    "custom_img_tag = \"latest\"\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "# collect default subnet IDs to deploy Sagemaker training job into\n",
    "ec2_session = boto3.Session(region_name=region)\n",
    "ec2_resource = ec2_session.resource(\"ec2\")\n",
    "subnet_ids = []\n",
    "for vpc in ec2_resource.vpcs.all():\n",
    "    # here you can choose which subnet based on the id\n",
    "    if vpc.is_default == True:\n",
    "        for subnet in vpc.subnets.all():\n",
    "            if subnet.default_for_az == True:\n",
    "                subnet_ids.append(subnet.id)\n",
    "\n",
    "# Retrieve XGBoost custom container from ECR registry path \n",
    "# custom_img_uri = account_id+\".dkr.ecr.\"+region+\".amazonaws.com/\"+custom_img_name+\":\"+custom_img_tag\n",
    "custom_img_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{custom_img_name}:{custom_img_tag}\"\n",
    "print(f\"\\nusing custom image: {custom_img_uri}\")\n",
    "\n",
    "# Create Sagemaker Estimator\n",
    "xgb_script_mode_estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri = custom_img_uri,\n",
    "    role=role,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    output_path=\"s3://{}/{}/output\".format(bucket, prefix),\n",
    "    sagemaker_session=sm_session,\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"./src\",\n",
    "    hyperparameters=hyperparams,\n",
    "    environment=env,\n",
    "    subnets = subnet_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf2f6891",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: xgboost-ddp-training-custom-2023-01-24-06-36-58-181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-24 06:36:58 Starting - Starting the training job...\n",
      "2023-01-24 06:37:15 Starting - Preparing the instances for training......\n",
      "2023-01-24 06:38:22 Downloading - Downloading input data\n",
      "2023-01-24 06:38:22 Training - Downloading the training image...\n",
      "2023-01-24 06:38:53 Training - Training image download completed. Training in progress....\u001b[34m[2023-01-24 06:39:13.954 ip-10-2-231-100.ec2.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-01-24:06:39:13:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2023-01-24:06:39:13:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2023-01-24:06:39:13:INFO] Invoking user training script.\u001b[0m\n",
      "\u001b[34m[2023-01-24:06:39:14:INFO] Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m[2023-01-24:06:39:14:INFO] Generating setup.cfg\u001b[0m\n",
      "\u001b[34m[2023-01-24:06:39:14:INFO] Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m[2023-01-24:06:39:14:INFO] Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train\n",
      "  Building wheel for train (setup.py): started\n",
      "  Building wheel for train (setup.py): finished with status 'done'\n",
      "  Created wheel for train: filename=train-1.0.0-py2.py3-none-any.whl size=10775 sha256=37799d7e102d56e1e22cdc1d7b63da6c6c6ddb303107bc24f360560c98a9edb6\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-bo947azi/wheels/f3/75/57/158162e9eab7af12b5c338c279b3a81f103b89d74eeb911c00\u001b[0m\n",
      "\u001b[34mSuccessfully built train\u001b[0m\n",
      "\u001b[34mInstalling collected packages: train\u001b[0m\n",
      "\u001b[34mSuccessfully installed train-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 22.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[2023-01-24:06:39:15:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2023-01-24:06:39:15:INFO] Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_xgboost_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"eta\": \"0.2\",\n",
      "        \"gamma\": \"4\",\n",
      "        \"max_depth\": \"5\",\n",
      "        \"min_child_weight\": \"6\",\n",
      "        \"num_round\": \"50\",\n",
      "        \"objective\": \"reg:squarederror\",\n",
      "        \"subsample\": \"0.7\",\n",
      "        \"verbosity\": \"2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"xgboost-ddp-training-custom-2023-01-24-06-36-58-181\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-328296961357/xgboost-ddp-training-custom-2023-01-24-06-36-58-181/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"eta\":\"0.2\",\"gamma\":\"4\",\"max_depth\":\"5\",\"min_child_weight\":\"6\",\"num_round\":\"50\",\"objective\":\"reg:squarederror\",\"subsample\":\"0.7\",\"verbosity\":\"2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_xgboost_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-328296961357/xgboost-ddp-training-custom-2023-01-24-06-36-58-181/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_xgboost_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"eta\":\"0.2\",\"gamma\":\"4\",\"max_depth\":\"5\",\"min_child_weight\":\"6\",\"num_round\":\"50\",\"objective\":\"reg:squarederror\",\"subsample\":\"0.7\",\"verbosity\":\"2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"xgboost-ddp-training-custom-2023-01-24-06-36-58-181\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-328296961357/xgboost-ddp-training-custom-2023-01-24-06-36-58-181/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--eta\",\"0.2\",\"--gamma\",\"4\",\"--max_depth\",\"5\",\"--min_child_weight\",\"6\",\"--num_round\",\"50\",\"--objective\",\"reg:squarederror\",\"--subsample\",\"0.7\",\"--verbosity\",\"2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_ETA=0.2\u001b[0m\n",
      "\u001b[34mSM_HP_GAMMA=4\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_DEPTH=5\u001b[0m\n",
      "\u001b[34mSM_HP_MIN_CHILD_WEIGHT=6\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_ROUND=50\u001b[0m\n",
      "\u001b[34mSM_HP_OBJECTIVE=reg:squarederror\u001b[0m\n",
      "\u001b[34mSM_HP_SUBSAMPLE=0.7\u001b[0m\n",
      "\u001b[34mSM_HP_VERBOSITY=2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/miniconda3/bin:/:/miniconda3/lib/python/site-packages/xgboost/dmlc-core/tracker:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m train --eta 0.2 --gamma 4 --max_depth 5 --min_child_weight 6 --num_round 50 --objective reg:squarederror --subsample 0.7 --verbosity 2\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/site-packages/snowflake/connector/options.py:96: UserWarning: You have an incompatible version of 'pyarrow' installed (1.0.1), please install a version that adheres to: 'pyarrow<8.1.0,>=8.0.0; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\u001b[0m\n",
      "\u001b[34msf_user=sermolinlive, sf_password=****, sf_account=uub56114.us-east-1, sf_warehouse=amazon_sagemake_w_snowflake_as_datasource, sf_database=HOUSING, sf_schema=HOUSING_SCHEMA, sf_protocol=https\u001b[0m\n",
      "\u001b[34mSnowflake connection established...\u001b[0m\n",
      "\u001b[34mgoing to download a random 50% sample of the data\u001b[0m\n",
      "\u001b[34msql=select * from CALIFORNIA_HOUSING sample (50);\u001b[0m\n",
      "\u001b[35m[2023-01-24 06:39:17.140 ip-10-2-203-230.ec2.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2023-01-24:06:39:17:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[35m[2023-01-24:06:39:17:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2023-01-24:06:39:17:INFO] Invoking user training script.\u001b[0m\n",
      "\u001b[35m[2023-01-24:06:39:17:INFO] Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[35mGenerating setup.py\u001b[0m\n",
      "\u001b[35m[2023-01-24:06:39:17:INFO] Generating setup.cfg\u001b[0m\n",
      "\u001b[35m[2023-01-24:06:39:17:INFO] Generating MANIFEST.in\u001b[0m\n",
      "\u001b[35m[2023-01-24:06:39:17:INFO] Installing module with the following command:\u001b[0m\n",
      "\u001b[35m/miniconda3/bin/python3 -m pip install . \u001b[0m\n",
      "\u001b[35mProcessing /opt/ml/code\n",
      "  Preparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mread data into a dataframe of shape (10376, 9)\u001b[0m\n",
      "\u001b[34mNumber of rows after dropping NA: 10376\u001b[0m\n",
      "\u001b[34mfinal shape of dataframe to be used for training (10376, 9)\u001b[0m\n",
      "\u001b[34mbuilding training and testing datasets\u001b[0m\n",
      "\u001b[34mfeatures=['MEDINC', 'HOUSEAGE', 'AVEROOMS', 'AVEBEDRMS', 'POPULATION', 'AVEOCCUP', 'LATITUDE', 'LONGITUDE']\u001b[0m\n",
      "\u001b[34mgoing to train the model\u001b[0m\n",
      "\u001b[35m  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: train\n",
      "  Building wheel for train (setup.py): started\n",
      "  Building wheel for train (setup.py): finished with status 'done'\n",
      "  Created wheel for train: filename=train-1.0.0-py2.py3-none-any.whl size=10775 sha256=46806002e2babeb41fc2e9c8ec30de4ff7ef97536f75bfa9cceb20868775c105\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-7_1g2a_e/wheels/f3/75/57/158162e9eab7af12b5c338c279b3a81f103b89d74eeb911c00\u001b[0m\n",
      "\u001b[35mSuccessfully built train\u001b[0m\n",
      "\u001b[35mInstalling collected packages: train\u001b[0m\n",
      "\u001b[35mSuccessfully installed train-1.0.0\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m[notice] A new release of pip available: 22.2.2 -> 22.3.1\u001b[0m\n",
      "\u001b[35m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[35m[2023-01-24:06:39:18:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2023-01-24:06:39:18:INFO] Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"framework_module\": \"sagemaker_xgboost_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"eta\": \"0.2\",\n",
      "        \"gamma\": \"4\",\n",
      "        \"max_depth\": \"5\",\n",
      "        \"min_child_weight\": \"6\",\n",
      "        \"num_round\": \"50\",\n",
      "        \"objective\": \"reg:squarederror\",\n",
      "        \"subsample\": \"0.7\",\n",
      "        \"verbosity\": \"2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": false,\n",
      "    \"job_name\": \"xgboost-ddp-training-custom-2023-01-24-06-36-58-181\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-328296961357/xgboost-ddp-training-custom-2023-01-24-06-36-58-181/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.m5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"eta\":\"0.2\",\"gamma\":\"4\",\"max_depth\":\"5\",\"min_child_weight\":\"6\",\"num_round\":\"50\",\"objective\":\"reg:squarederror\",\"subsample\":\"0.7\",\"verbosity\":\"2\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_xgboost_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-1-328296961357/xgboost-ddp-training-custom-2023-01-24-06-36-58-181/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_xgboost_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"eta\":\"0.2\",\"gamma\":\"4\",\"max_depth\":\"5\",\"min_child_weight\":\"6\",\"num_round\":\"50\",\"objective\":\"reg:squarederror\",\"subsample\":\"0.7\",\"verbosity\":\"2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"xgboost-ddp-training-custom-2023-01-24-06-36-58-181\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-328296961357/xgboost-ddp-training-custom-2023-01-24-06-36-58-181/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--eta\",\"0.2\",\"--gamma\",\"4\",\"--max_depth\",\"5\",\"--min_child_weight\",\"6\",\"--num_round\",\"50\",\"--objective\",\"reg:squarederror\",\"--subsample\",\"0.7\",\"--verbosity\",\"2\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_HP_ETA=0.2\u001b[0m\n",
      "\u001b[35mSM_HP_GAMMA=4\u001b[0m\n",
      "\u001b[35mSM_HP_MAX_DEPTH=5\u001b[0m\n",
      "\u001b[35mSM_HP_MIN_CHILD_WEIGHT=6\u001b[0m\n",
      "\u001b[35mSM_HP_NUM_ROUND=50\u001b[0m\n",
      "\u001b[35mSM_HP_OBJECTIVE=reg:squarederror\u001b[0m\n",
      "\u001b[35mSM_HP_SUBSAMPLE=0.7\u001b[0m\n",
      "\u001b[35mSM_HP_VERBOSITY=2\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/miniconda3/bin:/:/miniconda3/lib/python/site-packages/xgboost/dmlc-core/tracker:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35m/miniconda3/bin/python3 -m train --eta 0.2 --gamma 4 --max_depth 5 --min_child_weight 6 --num_round 50 --objective reg:squarederror --subsample 0.7 --verbosity 2\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.8/site-packages/snowflake/connector/options.py:96: UserWarning: You have an incompatible version of 'pyarrow' installed (1.0.1), please install a version that adheres to: 'pyarrow<8.1.0,>=8.0.0; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\u001b[0m\n",
      "\u001b[35msf_user=sermolinlive, sf_password=****, sf_account=uub56114.us-east-1, sf_warehouse=amazon_sagemake_w_snowflake_as_datasource, sf_database=HOUSING, sf_schema=HOUSING_SCHEMA, sf_protocol=https\u001b[0m\n",
      "\u001b[35mSnowflake connection established...\u001b[0m\n",
      "\u001b[35mgoing to download a random 50% sample of the data\u001b[0m\n",
      "\u001b[35msql=select * from CALIFORNIA_HOUSING sample (50);\u001b[0m\n",
      "\u001b[35mread data into a dataframe of shape (10302, 9)\u001b[0m\n",
      "\u001b[35mNumber of rows after dropping NA: 10302\u001b[0m\n",
      "\u001b[35mfinal shape of dataframe to be used for training (10302, 9)\u001b[0m\n",
      "\u001b[35mbuilding training and testing datasets\u001b[0m\n",
      "\u001b[35mfeatures=['MEDINC', 'HOUSEAGE', 'AVEROOMS', 'AVEBEDRMS', 'POPULATION', 'AVEOCCUP', 'LATITUDE', 'LONGITUDE']\u001b[0m\n",
      "\u001b[35mgoing to train the model\u001b[0m\n",
      "\u001b[34m[06:39:21] task NULL got new rank 1\u001b[0m\n",
      "\u001b[35m[06:39:21] task NULL got new rank 0\u001b[0m\n",
      "\u001b[34m[06:39:25] task NULL got new rank 1\u001b[0m\n",
      "\u001b[34m[06:39:25] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:25] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:25] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:25] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:25] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:25] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:25] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:25] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:25] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:25] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:25] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:25] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:25] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:25] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 48 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:25] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:25] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 50 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:25] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:25] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:25] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:25] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 48 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:25] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:25] task NULL got new rank 0\u001b[0m\n",
      "\u001b[35m[06:39:25] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:25] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:25] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:25] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:25] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:25] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:25] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:25] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:25] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:25] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:25] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:25] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:25] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:25] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 48 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:25] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:25] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 50 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:25] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:25] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:25] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:25] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 48 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:25] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:25] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:25] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:25] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:25] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:25] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 36 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:25] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:25] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 40 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:25] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 38 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 24 extra nodes, 38 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 40 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 26 extra nodes, 32 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 20 extra nodes, 36 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 32 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 30 extra nodes, 32 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 30 extra nodes, 28 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 16 extra nodes, 32 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 28 extra nodes, 28 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 26 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m[06:39:25] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:25] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:25] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:25] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:25] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 36 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:25] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:25] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 40 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:25] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 38 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 24 extra nodes, 38 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 40 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 26 extra nodes, 32 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 20 extra nodes, 36 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 32 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 30 extra nodes, 32 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 30 extra nodes, 28 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 16 extra nodes, 32 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 28 extra nodes, 28 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 26 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 20 extra nodes, 38 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 26 extra nodes, 30 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 32 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 22 extra nodes, 38 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 10 extra nodes, 34 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 38 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 24 extra nodes, 38 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 32 extra nodes, 30 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 12 extra nodes, 34 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 22 extra nodes, 30 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 20 extra nodes, 30 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 18 extra nodes, 42 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 26 extra nodes, 36 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 24 extra nodes, 34 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 10 extra nodes, 30 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 10 extra nodes, 38 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 12 extra nodes, 28 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 20 extra nodes, 38 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 26 extra nodes, 30 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 32 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 22 extra nodes, 38 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:26] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 10 extra nodes, 34 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 38 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 24 extra nodes, 38 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 32 extra nodes, 30 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 12 extra nodes, 34 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[35m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 22 extra nodes, 30 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 20 extra nodes, 30 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 18 extra nodes, 42 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 26 extra nodes, 36 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 24 extra nodes, 34 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 10 extra nodes, 30 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 10 extra nodes, 38 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 12 extra nodes, 28 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 20 extra nodes, 30 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 20 extra nodes, 30 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 12 extra nodes, 34 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 20 extra nodes, 34 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 18 extra nodes, 36 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[06:39:28] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 10 extra nodes, 36 pruned nodes, max_depth=5\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 12 extra nodes, 34 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 20 extra nodes, 34 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 18 extra nodes, 36 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[06:39:27] INFO: ../src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[06:39:28] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 10 extra nodes, 36 pruned nodes, max_depth=5\u001b[0m\n",
      "\n",
      "2023-01-24 06:39:33 Uploading - Uploading generated training model\n",
      "2023-01-24 06:40:11 Completed - Training job completed\n",
      "Training seconds: 246\n",
      "Billable seconds: 246\n"
     ]
    }
   ],
   "source": [
    "# Estimator fitting\n",
    "xgb_script_mode_estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ee9c554",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the trained model is available in S3 -> s3://sagemaker-us-east-1-328296961357/sagemaker/sagemaker-snowflake-example/output/xgboost-ddp-training-custom-2023-01-24-06-36-58-181/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print(f\"the trained model is available in S3 -> {xgb_script_mode_estimator.model_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1439033c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Remember that the training job that we ran is very \"light\", due to the very small dataset. As such, running locally on the notebook instance results in a faster execution time, compared to SageMaker. SageMaker takes longer time to run the job because it has to provision the training infrastructure. Since this example training job not very resource-intensive, the infrastructure provisioning process adds more overhead, compared to the training job itself.\n",
    "\n",
    "In a real situation, where datasets are large, running on SageMaker can considerably speed up the execution process - and help us optimize costs, by keeping this interactive notebook environment modest and spinning up more powerful training job resources on-demand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c252de2c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleaning up\n",
    "\n",
    "To avoid incurring future charges, delete the resources. You can do this by deleting the cloud formation template used to create the IAM role and the Amazon SageMaker Notebook.\n",
    "![Cleaning Up](img/cfn-delete.png)\n",
    "\n",
    "You will have to delete the Snowflake resources manually from the Snowflake console."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0782db29",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this notebook we saw how to download data stored in Snowflake table to Sagemaker Training job instance and train a XGBoost model using a custom training container. **This approach allows us to directly integrate Snowflake as a data source with Sagemaker notebook without having the data staged on S3.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd0f64d-fbf4-4260-a934-9407d629c9db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "c788421e88f093f8dd506f4527624e47e75432cd7217f9d7714b2cd296741f7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
